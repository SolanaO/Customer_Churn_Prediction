{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc967a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\": {\"spark.pyspark.python\":\"/home/hadoop/path/to/venv/bin/python3\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a7375",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> Sparkify Full Dataset Models and Evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b9c81",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This notebook is dedicated to the following tasks:\n",
    "    <li>load and clean data</li>\n",
    "    <li>pre-process data and create new features</li>\n",
    "    <li>stratified split features dataset into train and test sets</li>\n",
    "    <li>fit several classifiers on train set, using default parameters</li>\n",
    "    <li>tune two best classifiers using grid search and cross validation</li>\n",
    "    <li>evaluate metrics and display all the results</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40a332",
   "metadata": {},
   "source": [
    "## <font color='blue'>Set Environment</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PySpark libraries and packages\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    IntegerType, \n",
    "    DateType, \n",
    "    TimestampType,\n",
    "    )\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    min as Fmin, max as Fmax, \n",
    "    sum as Fsum, round as Fround, \n",
    "    \n",
    "    col, lit, \n",
    "    first, last, \n",
    "    desc, asc,\n",
    "    avg, count, countDistinct, \n",
    "    when, isnull, isnan,\n",
    "    from_unixtime, \n",
    "    datediff,\n",
    "    )\n",
    "\n",
    "# libraries and packages for modeling\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder, \n",
    "    OneHotEncoderModel\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier,\n",
    "    MultilayerPerceptronClassifier,\n",
    "    LinearSVC,\n",
    "    NaiveBayes\n",
    ")\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a Spark session using the SparkSession APIs\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Sparkify\")\n",
    "        .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the available memory in the cluster\n",
    "print('Available AWS-EMR Memory: {}'.format(spark.sparkContext\n",
    "                                            .getConf().get('spark.driver.memory')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0794133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import sklearn metrics related packages\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for enhanced plotting\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "colors = sns.color_palette('PuBuGn_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adeb453",
   "metadata": {},
   "source": [
    "## <font color='blue'>Load, Preprocess, Process and Split Dataset Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df529952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the raw dataset in Spark.\n",
    "    \n",
    "    INPUT:\n",
    "            (str) - path for datafile\n",
    "    OUTPUT:\n",
    "            (PySpark dataframe) - dataframe of raw dat\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading the dataset ...\")\n",
    "    df = spark.read.json(file_path)\n",
    "    print(\"Dataset is loaded...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_data(df, data_path):\n",
    "    \"\"\"\n",
    "    Saves the PySpark dataframe to a file.\n",
    "    \n",
    "    INPUT:\n",
    "            df (PySpark dataframe) - data to be saved\n",
    "            data_path (str) - path for datafile\n",
    "    OUTPUT:\n",
    "            none\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df.write.json(data_path)\n",
    "    \n",
    "    \n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Performs basic cleaning operations on the raw data:\n",
    "        - removes entries with missing userId\n",
    "        - rescale timestamp columns to seconds\n",
    "        - drop unnecesary columns\n",
    "            - personal information columns\n",
    "            - song information columns\n",
    "            - web and browser information\n",
    "            - timestamp columns in miliseconds\n",
    "\n",
    "    INPUT:\n",
    "        (PySpark dataframe) - dataframe of raw data\n",
    "    OUTPUT:\n",
    "        (PySpark dataframe) - dataframe of cleaned data\n",
    "    \"\"\"\n",
    "\n",
    "    # print message to indicate the start of the process\n",
    "    print(\"Cleaning the data ...\")\n",
    "\n",
    "    # print a count of rows before cleaning\n",
    "    initial_records = df.count()\n",
    "    print(\"Dataset has {} rows initially.\".format(initial_records))\n",
    "\n",
    "    # filter out all the records without userId\n",
    "    df = df.where(df.userId != \"\")\n",
    "\n",
    "    # rescale the timestamp to seconds (initially in miliseconds)\n",
    "    df = df.withColumn(\"log_ts\", df.ts/1000.0)\n",
    "    df = df.withColumn(\"reg_ts\", df.registration/1000.0)\n",
    "\n",
    "    # drop several unnecessary columns\n",
    "    cols_to_drop = (\"firstName\", \"lastName\", \"location\",\n",
    "                    \"artist\", \"song\", \"length\",\n",
    "                    \"userAgent\", \"method\", \"status\",\n",
    "                    \"ts\", \"registration\"\n",
    "                   )\n",
    "    df = df.drop(*cols_to_drop)\n",
    "\n",
    "\n",
    "    # print end of process message\n",
    "    print(\"Finished cleaning the data ...\")\n",
    "\n",
    "    # print a count of rows after cleaning\n",
    "    removed_rows = initial_records - df.count()\n",
    "\n",
    "    print(\"Cleaned dataset has {} rows, {} rows were removed\". \\\n",
    "        format(df.count(), initial_records - df.count()))\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare the data for modeling via creating several features.\n",
    "\n",
    "        - reg_date (date) - month-year of the registration\n",
    "\n",
    "        - create windows grouped on userId and sessionId\n",
    "\n",
    "         - firstevent_ts (timestamp) - first time an user is active\n",
    "         - lastevent_ts (timestamp) - last time an user is active\n",
    "\n",
    "         - init_days_interv (float) - days between registration and first activity\n",
    "         - tenure_days_interv (float) - days between registration and last activity\n",
    "         - active_days (float) - days the user has some activity on the platform\n",
    "         - session_h (float) - session's duration in hours\n",
    "\n",
    "     INPUT:\n",
    "         df (PySpark dataframe) - cleaned dataframe\n",
    "     OUTPUT:\n",
    "         df (PySpark dataframe) - dataframe with the listed features added\n",
    "    \"\"\"\n",
    "\n",
    "    # extract registration month and year from timestamp\n",
    "    df = df.withColumn(\"reg_date\", from_unixtime(col(\"reg_ts\"), \"MM-yyyy\"))\n",
    "\n",
    "    # create window: data grouped by userId, time ordered\n",
    "    win_user = (W.partitionBy(\"userId\")\n",
    "            .orderBy(\"log_ts\")\n",
    "            .rangeBetween(W.unboundedPreceding, W.unboundedFollowing))\n",
    "\n",
    "    # create window: data grouped by sessionId and userId, time ordered\n",
    "    win_user_session = (W.partitionBy(\"sessionId\", \"userId\")\n",
    "                        .orderBy(\"log_ts\")\n",
    "                        .rangeBetween(W.unboundedPreceding, W.unboundedFollowing))\n",
    "\n",
    "    # record the first time an user is active\n",
    "    df = df.withColumn(\"firstevent_ts\", first(col(\"log_ts\")).over(win_user))\n",
    "    # record the last time an user is active\n",
    "    df = df.withColumn(\"lastevent_ts\", last(col(\"log_ts\")).over(win_user))\n",
    "\n",
    "    # warmup time = registration time to first event in days\n",
    "    df = df.withColumn(\"init_days_interv\",\n",
    "                       (col(\"firstevent_ts\").cast(\"long\")-col(\"reg_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # tenure time = registration time to last event in days\n",
    "    df = df.withColumn(\"tenure_days_interv\",\n",
    "                       (col(\"lastevent_ts\").cast(\"long\")-col(\"reg_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # active time =  days between the first event and the last event in days\n",
    "    df = df.withColumn(\"active_days\",\n",
    "                       (col(\"lastevent_ts\").cast(\"long\")-col(\"firstevent_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # create column that records the individual session's duration in hours\n",
    "    df = df.withColumn(\"session_h\",\n",
    "                    (last(df.log_ts).over(win_user_session) \\\n",
    "                     - first(df.log_ts).over(win_user_session))/3600)\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(\"reg_ts\", \"log_ts\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_features(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Features engineered to be used in modelling.\n",
    "\n",
    "        - nr_songs (int) - total number of songs user listened to\n",
    "        - nr_playlist (int) - number of songs added to the playlist\n",
    "\n",
    "        - nr_friends (int) - number of friends added through \"Add Friend\"\n",
    "\n",
    "        - nr_likes (int) - total number of \"Thumbs Up\" of the user\n",
    "        - nr_dislikes (int) - total number of \"Thumbs Down\" of the user\n",
    "\n",
    "        - nr_downgrades (int) - total number of visits to \"Downgrade\" page by the user\n",
    "        - nr_upgrades (int) - total number of visits to \"Upgrade\" page by the user\n",
    "\n",
    "        - nr_home (int) - total number of visits to \"Home\" page by the user\n",
    "        - nr_settings (int) - total number of visits to \"Settings\" page by the user\n",
    "\n",
    "        - nr_error (int) - total number of errors encountered by the user\n",
    "\n",
    "        - nr_ads (int) - total number of ads the user got\n",
    "        - nr_sessions (int) - number of sessions of the user\n",
    "        - n_acts (int) - total number of actions taken by the user\n",
    "\n",
    "        - avg_sess_h (float) - average session length in hours\n",
    "        - acts_per_session (float) - average number of actions per session for the user\n",
    "        - songs_per_session (float) - average numer of songs listened per session by the user\n",
    "        - ads_per_session (float) - average number of ads per session, received by user\n",
    "\n",
    "        - init_days_interv (int) - time interval in days from registration to the first action of the user\n",
    "        - tenure_days_interv (int) - time interval in days from registration to the last action of the user\n",
    "        - active_days (int) - number of days the user was active on the platform\n",
    "\n",
    "        - gender (binary) - 1 for F (female), 0 for M (male)\n",
    "        - level (binary) - 1 for paid, 0 for free\n",
    "\n",
    "        - churn (binary) - 1 for \"Cancellation Confirmation\" page visit, 0 otherwise\n",
    "\n",
    "    INPUT:\n",
    "        df (PySpark dataframe) - preprocessed dataframe\n",
    "    OUTPUT:\n",
    "        df_feats (PySpark dataframe) - dataframe that contains engineered features\n",
    "    \"\"\"\n",
    "\n",
    "    df_feats = df.groupBy(\"userId\") \\\n",
    "        .agg(\n",
    "\n",
    "            # count user's individual actions using all page visits\n",
    "\n",
    "            count(when(col(\"page\") == \"NextSong\", True)).alias(\"nr_songs\"),\n",
    "            count(when(col(\"page\") == \"Add to Playlist\", True)).alias(\"nr_playlist\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Add Friend\", True)).alias(\"nr_friends\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Thumbs Up\", True)).alias(\"nr_likes\"),\n",
    "            count(when(col(\"page\") == \"Thumbs Down\", True)).alias(\"nr_dislikes\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Downgrade\", True)).alias(\"nr_downgrades\"),\n",
    "            count(when(col(\"page\") == \"Upgrade\", True)).alias(\"nr_upgrades\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Home\", True)).alias(\"nr_home\"),\n",
    "            count(when(col(\"page\") == \"Settings\", True)).alias(\"nr_settings\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Error\", True)).alias(\"nr_error\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Roll Advert\", True)).alias(\"nr_ads\"),\n",
    "\n",
    "            # compute the number of sessions a user is in\n",
    "            countDistinct(\"sessionId\").alias(\"nr_sessions\"),\n",
    "\n",
    "            # find the total number of actions a user took\n",
    "            countDistinct(\"itemInSession\").alias(\"n_acts\"),\n",
    "\n",
    "            # compute the average session length in hours\n",
    "            avg(col(\"session_h\")).alias(\"avg_sess_h\"),\n",
    "\n",
    "            # compute the average number of page actions per sesssion - i.e. items in session\n",
    "            (countDistinct(\"itemInSession\")/countDistinct(\"sessionId\")).alias(\"acts_per_session\"),\n",
    "\n",
    "            # compute the average number of songs per session\n",
    "            (count(when(col(\"page\") == \"NextSong\",\n",
    "                        True))/countDistinct(\"sessionId\")).alias(\"songs_per_session\"),\n",
    "\n",
    "            # compute the average number of ads per session\n",
    "             (count(when(col(\"page\") == \"Roll Advert\",\n",
    "                         True))/countDistinct(\"sessionId\")).alias(\"ads_per_session\"),\n",
    "\n",
    "            # days between registration and first activity\n",
    "            first(col(\"init_days_interv\")).alias(\"init_days_interv\"),\n",
    "            # the tenure time on the platform: from registration to last event in days\n",
    "            first(col(\"tenure_days_interv\")).alias(\"tenure_days_interv\"),\n",
    "            # number of days user visited the platform, is active on the platform\n",
    "            first(col(\"active_days\")).alias(\"active_days\"),\n",
    "\n",
    "            # encode the gender 1 for F and 0 for M\n",
    "            first(when(col(\"gender\") == \"F\", 1).otherwise(0)).alias(\"gender\"),\n",
    "\n",
    "            # encode the level (paid/free) according to the last record\n",
    "            last(when(col(\"level\") == \"paid\", 1).otherwise(0)).alias(\"level\"),\n",
    "\n",
    "            # flag those users that downgraded\n",
    "            #last(when(col(\"page\") == \"Downgrade\", 1).otherwise(0)).alias(\"downgrade\"),\n",
    "\n",
    "            # create the churn column that records if the user cancelled\n",
    "            last(when(col(\"page\") == \"Cancellation Confirmation\", 1).otherwise(0)).alias(\"churn\"),\n",
    "            )\n",
    "\n",
    "    # columns to drop\n",
    "    drop_cols = (\"userId\", \"gender\", \"avg_sess_h\",\n",
    "                 \"nr_playlist\", \"nr_home\")\n",
    "    # drop the columns\n",
    "    #df_feats = df_feats.drop(\"userId\")\n",
    "    df_feats = df_feats.drop(*drop_cols)\n",
    "\n",
    "    # drop the null values\n",
    "    df_feats=df_feats.na.drop()\n",
    "\n",
    "    return df_feats\n",
    "\n",
    "\n",
    "SPLIT_VALS = [.7, .3]\n",
    "\n",
    "# split the data into train and test sets\n",
    "\n",
    "def split_data (df):\n",
    "\n",
    "    \"\"\"\n",
    "    Split the dataset into training set and test set.\n",
    "    Use a stratified sampling method.\n",
    "\n",
    "    INPUT:\n",
    "        df (PySpark dataframe) - dataframe\n",
    "    OUTPUT:\n",
    "        train_set, test_set (PySpark dataframes) - percentage split based on the provided values\n",
    "    \"\"\"\n",
    "\n",
    "    # split dataframes between 0s and 1s\n",
    "    zeros = df.filter(df[\"churn\"]==0)\n",
    "    ones = df.filter(df[\"churn\"]==1)\n",
    "\n",
    "    # split dataframes into training and testing\n",
    "    train0, test0 = zeros.randomSplit(SPLIT_VALS, seed=1234)\n",
    "    train1, test1 = ones.randomSplit(SPLIT_VALS, seed=1234)\n",
    "\n",
    "    # stack datasets back together\n",
    "    train_set = train0.union(train1)\n",
    "    test_set = test0.union(test1)\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d15df",
   "metadata": {},
   "source": [
    "## <font color='blue'>Load and Clean Data, Create Feature Dataset and Split Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ed59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST\n",
    "# path for the train set file\n",
    "path_mini = \"data/mini_sparkify_event_data.json\"\n",
    "\n",
    "# upload the train data\n",
    "df = load_data(path_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path variable for the full dataset file\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b48295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# load the dataset \n",
    "df = load_data(event_data)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "load_time = (end_time - start_time)/60\n",
    "\n",
    "# print the loading time\n",
    "print(\"\")\n",
    "print(\"Load time.........%6.3f min\" % load_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set memory protocol for spark\n",
    "df_cached = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ea64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# clean the raw dataset\n",
    "df_clean = clean_data(df_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "clean_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Cleaning time.........%6.3f min\" % clean_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# preprocess the dataset\n",
    "df_proc = preprocess_data(df_clean)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "preproc_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Preprocessing time.........%6.3f min\" % preproc_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# create features dataset\n",
    "df_feats = build_features(df_proc)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "feat_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Features engineering time.........%6.3f min\" % feat_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67065be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the features dataset\n",
    "df_train, df_test = split_data(df_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3d61b",
   "metadata": {},
   "source": [
    "### <font color='blue'>Toggle the memory</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the train set to the memory\n",
    "df_cached.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c197651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the train set to the memory\n",
    "train_cached = df_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the test set to the memory\n",
    "test_cached = df_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60950aa7",
   "metadata": {},
   "source": [
    "## <font color='blue'>Baseline Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd200879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of churn users in each set\n",
    "fn_train = train_cached.where(train_cached.churn==1).count()\n",
    "fn_test = test_cached.where(test_cached.churn==1).count()\n",
    "\n",
    "# count the number of not churn users in each set\n",
    "tn_train = train_cached.where(train_cached.churn==0).count()\n",
    "tn_test = test_cached.where(test_cached.churn==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of ZeroR model on train set \n",
    "accuracy_train = tn_train/(fn_train+tn_train)\n",
    "# print the result\n",
    "print(\"With fp = tp = 0, fn = {} and tn = {}, the accuracy of the ZeroR model on the train set is {}%.\"\n",
    "      .format(fn_train, tn_train, round(accuracy_train,4)*100))\n",
    "\n",
    "# accuracy of ZeroR model on test set\n",
    "accuracy_test = tn_test/(fn_test+tn_test)\n",
    "# print the result\n",
    "print(\"With fp = tp = 0, fn = {} and tn = {}, the accuracy of the ZeroR model on the test set is {}%.\"\n",
    "     .format(fn_test, tn_test, round(accuracy_test,4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd4b3e",
   "metadata": {},
   "source": [
    "### <font color='blue'>Build Models Evaluators</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ea7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute relevant metrics for binary classification\n",
    "def conf_metrics(dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "        Calculates the metrics associated to the confusion matrix.\n",
    "\n",
    "        INPUT:\n",
    "            dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "        OUTPUT:\n",
    "            accuracy (float) - metric\n",
    "            precision (float) - metric\n",
    "            recall (float) - metric\n",
    "            F1 (float) - metric\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # calculate the elements of the confusion matrix\n",
    "    tn = dataset.where((dataset[labelCol]==0) & (dataset[predCol]==0)).count()\n",
    "    tp = dataset.where((dataset[labelCol]==1) & (dataset[predCol]==1)).count()                   \n",
    "    fn = dataset.where((dataset[labelCol]==1) & (dataset[predCol]==0)).count()                   \n",
    "    fp = dataset.where((dataset[labelCol]==0) & (dataset[predCol]==1)).count()\n",
    "    \n",
    "    # calculate accuracy, precision, recall, and F1-score\n",
    "    accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 =  2 * (precision*recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc242b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display the metrics of interest\n",
    "def display_metrics(dataset, roc_cl, pr_cl):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints evaluation metrics for the model. \n",
    "    \n",
    "    INPUT:\n",
    "         dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = conf_metrics(dataset)[0]\n",
    "    precision = conf_metrics(dataset)[1]\n",
    "    recall = conf_metrics(dataset)[2]\n",
    "    f1 = conf_metrics(dataset)[3]\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    dataset.groupBy(dataset[labelCol], dataset[predCol]).count().show()\n",
    "    print(\"\")\n",
    "    print(\"accuracy...............%6.3f\" % accuracy)\n",
    "    print(\"precision..............%6.3f\" % precision)\n",
    "    print(\"recall.................%6.3f\" % recall)\n",
    "    print(\"F1.....................%6.3f\" % f1)\n",
    "    print(\"auc_roc................%6.3f\" % roc_cl)\n",
    "    print(\"auc_pr.................%6.3f\" % pr_cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64344a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot heatmap for a confusion matrix\n",
    "\n",
    "def conf_mat_heatmap(predictions, model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses confusion matrix utility in sklearn to print a heatmap.\n",
    "    \n",
    "    INPUT:\n",
    "        predictions (PySpark dataframe) - contains probability predictions, label column\n",
    "        model_name (str) - abreviated classifier name\n",
    "        \n",
    "    OUTPUT:\n",
    "        none - confusion matrix heatmap is displayed\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # transform predictions PySpark dataframe into Pandas dataframe\n",
    "    pred_pandas = predictions.select(predictions[labelCol], predictions[predCol]).toPandas()\n",
    "    \n",
    "    # to avoid stacking plots\n",
    "    plt.clf()\n",
    "    \n",
    "    # generate the confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(pred_pandas[labelCol], pred_pandas[predCol], \n",
    "                                            cmap=\"Blues\", values_format='d', ax=None)\n",
    "    \n",
    "    \n",
    "    # figure title\n",
    "    plt.title(\"Confusion Matrix:\" + model_name)\n",
    "\n",
    "    # display the plot\n",
    "    %matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the roc and pr curves side by side\n",
    "def plot_roc_pr_curves(predictions, model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates ROC-AUC and PR-AUC scores and plots the ROC and PR curves.\n",
    "    \n",
    "    INPUT:\n",
    "        predictions (PySpark dataframe) - contains probability predictions, label column\n",
    "        model_name (str) - classifier name\n",
    "        \n",
    "    OUTPUT:\n",
    "        none - two plots are displayed\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # transform predictions PySpark dataframe into Pandas dataframe\n",
    "    pred_pandas = predictions.select(predictions.label, predictions.probability).toPandas()\n",
    "    \n",
    "    # calculate roc_auc score\n",
    "    roc_auc = roc_auc_score(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(pred_pandas.label))]\n",
    "    # calculate roc curves\n",
    "    fpr, tpr, _ = roc_curve(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(pred_pandas.label, ns_probs)\n",
    "    \n",
    "    # calculate precision, recall for each threshold\n",
    "    precision, recall, _ = precision_recall_curve(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    # calculate pr auc score\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # to avoid stacking plots\n",
    "    plt.clf()\n",
    "    \n",
    "    # create figure which contains two subplots\n",
    "    plt.figure(figsize=[12,6])\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    \n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    plt.plot(fpr, tpr, marker='.', color=\"firebrick\", label='ROC AUC = %.3f' % (roc_auc))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"ROC Curve:\" + model_name)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    # plot the precision-recall curves\n",
    "    \n",
    "    ns_line = len(pred_pandas[pred_pandas.label==1]) / len(pred_pandas.label)\n",
    "    plt.plot([0, 1], [ns_line, ns_line], linestyle='--', label='No Skill')\n",
    "    plt.plot(recall, precision, marker='.', color=\"firebrick\", label='PR AUC = %.3f' % (pr_auc))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"Precision-Recall Curve:\" + model_name)\n",
    "\n",
    "    # display the plot\n",
    "    %matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c147aee",
   "metadata": {},
   "source": [
    "### <font color='blue'>Build Pipelines</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55156c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the features and the label\n",
    "CAT_FEATURES = [\"level\"]\n",
    "CONT_FEATURES = [\"nr_songs\", \"nr_likes\", \"nr_dislikes\", \"nr_friends\", \"nr_downgrades\",\n",
    "                \"nr_upgrades\", \"nr_error\", \"nr_settings\", \"nr_ads\", \"nr_sessions\",\n",
    "                \"n_acts\", \"acts_per_session\", \"songs_per_session\", \"ads_per_session\",\n",
    "                \"init_days_interv\", \"tenure_days_interv\", \"active_days\"]\n",
    "CHURN_LABEL = \"churn\"\n",
    "\n",
    "# create labels and features\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "featuresCol = \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_pipeline(classifier):\n",
    "    \"\"\"\n",
    "    Combines all the stages of the processing and modeling.\n",
    "    \"\"\"\n",
    "    # stages in the pipeline\n",
    "    stages = [] \n",
    "    \n",
    "    # encode the labels\n",
    "    label_indexer =  StringIndexer(inputCol=CHURN_LABEL, outputCol=\"label\")\n",
    "    stages += [label_indexer]\n",
    "    \n",
    "    # encode the binary features\n",
    "    bin_assembler = VectorAssembler(inputCols=CAT_FEATURES, outputCol=\"bin_features\")\n",
    "    stages += [bin_assembler]\n",
    "    \n",
    "    # encode the continuous features\n",
    "    cont_assembler = VectorAssembler(inputCols = CONT_FEATURES, outputCol=\"cont_features\")\n",
    "    stages += [cont_assembler]\n",
    "    # normalize the continuous features\n",
    "    cont_scaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaler\", \n",
    "                                 withStd=True , withMean=True)\n",
    "    stages += [cont_scaler]\n",
    "    \n",
    "    # pass all to the vector assembler to create a single sparse vector\n",
    "    all_assembler = VectorAssembler(inputCols=[\"bin_features\", \"cont_scaler\"],  \n",
    "                            outputCol=\"features\")\n",
    "    stages += [all_assembler]\n",
    "    \n",
    "    # add the model to the pipeline\n",
    "    stages += [classifier]\n",
    "    \n",
    "    # create a pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6de972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement K-fold cross validation and grid search \n",
    "\n",
    "def grid_search_model(pipeline, param):\n",
    "    \"\"\"\n",
    "    Creates a cross validation object and performs grid search\n",
    "    over a set of parameters.\n",
    "    \n",
    "    INPUT:\n",
    "        param = grid of parameters\n",
    "        pipeline = model pipeline \n",
    "    \n",
    "    OUTPUT:\n",
    "        cv = cross validation object\n",
    "    \"\"\"\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=param,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5,\n",
    "                    parallelism=2)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50124f",
   "metadata": {},
   "source": [
    "## <font color='blue'>Evaluate PySpark Classifiers</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a550beb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Fit the binary classifiers from PySpark on the train set with cross validation and default parameters to select the best ones for the next stage.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8461eef",
   "metadata": {},
   "source": [
    "### <font color='blue'>Logistic Regression Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b591c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training LOGISTIC REGRESSION CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "lr_classifier = LogisticRegression(labelCol = labelCol,\n",
    "                                       featuresCol = featuresCol)\n",
    "\n",
    "# build specific pipeline\n",
    "lr_pipeline = build_full_pipeline(lr_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_lr = lr_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_lr = model_lr.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_lr = evaluator.evaluate(predictions_lr, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_lr = evaluator.evaluate(predictions_lr, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_lr, prec_lr, rec_lr, f1_lr = conf_metrics(predictions_lr)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"LR: Train set evaluation metrics\")\n",
    "display_metrics(predictions_lr, roc_lr, pr_lr)\n",
    "print(\"\")\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_lr, \"LR\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_lr, \"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f85bac",
   "metadata": {},
   "source": [
    "### <font color='blue'>Decision Trees Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training DECISION TREES CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# build specific pipeline\n",
    "dt_classifier = DecisionTreeClassifier(labelCol = labelCol,\n",
    "                                           featuresCol = featuresCol,\n",
    "                                           seed=1234)\n",
    "dt_pipeline = build_full_pipeline(dt_classifier)\n",
    "\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_dt = dt_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_dt = model_dt.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_dt = evaluator.evaluate(predictions_dt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_dt = evaluator.evaluate(predictions_dt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_dt, prec_dt, rec_dt, f1_dt = conf_metrics(predictions_dt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"DT: Train set evaluation metrics\")\n",
    "display_metrics(predictions_dt, roc_dt, pr_dt)\n",
    "print(\"\")\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_dt, \"DT\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_dt, \"DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c3700",
   "metadata": {},
   "source": [
    "### <font color='blue'>Random Forest Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training RANDOM FOREST CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# instantiate the classifier\n",
    "rf_classifier = RandomForestClassifier(labelCol = labelCol,\n",
    "                                       featuresCol = featuresCol, \n",
    "                                       seed=1234)\n",
    "# build the specific pipeline\n",
    "rf_pipeline = build_full_pipeline(rf_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_rf = rf_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_rf = model_rf.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_rf, prec_rf, rec_rf, f1_rf = conf_metrics(predictions_rf)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"RF: Train set evaluation metrics\")\n",
    "display_metrics(predictions_rf, roc_rf, pr_rf)\n",
    "print(\"\")\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_rf, \"RF\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_rf, \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09ea7c",
   "metadata": {},
   "source": [
    "### <font color='blue'>Gradient Boosted Trees</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training GRADIENT BOOSTED TREES\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "gbt_classifier = GBTClassifier(labelCol = labelCol,\n",
    "                                featuresCol = featuresCol,\n",
    "                                seed=1234)\n",
    "# build specific pipeline\n",
    "gbt_pipeline = build_full_pipeline(gbt_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_gbt = gbt_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_gbt = model_gbt.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_gbt, prec_gbt, rec_gbt, f1_gbt = conf_metrics(predictions_gbt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"GBT: Train set evaluation metrics\")\n",
    "display_metrics(predictions_gbt, roc_gbt, pr_gbt)\n",
    "print(\"\")\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_gbt, \"GBT\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_gbt, \"GBT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc397ba",
   "metadata": {},
   "source": [
    "### <font color='blue'>Multilayer Perceptron Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c932fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training MULTILAYER PERCEPTRON CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "# specify layers: 18 (features), two intermediate (8, 4), output 2 (classes)\n",
    "layers=[18, 8, 4, 2]\n",
    "# create the trainer and set its parameters\n",
    "mlpc_classifier = MultilayerPerceptronClassifier(labelCol = labelCol,\n",
    "                                                featuresCol = featuresCol,\n",
    "                                                layers=layers,\n",
    "                                                seed=1234)\n",
    "# build specific pipeline\n",
    "mlpc_pipeline = build_full_pipeline(mlpc_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_mlpc = mlpc_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_mlpc = model_mlpc.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc = conf_metrics(predictions_mlpc)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"MLPC: Train set evaluation metrics\")\n",
    "display_metrics(predictions_mlpc, roc_mlpc, pr_mlpc)\n",
    "print(\"\")\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_mlpc, \"MLPC\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_mlpc, \"MLPC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9691f89",
   "metadata": {},
   "source": [
    "### <font color='blue'>Linear Support Vector Machine</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training LINEAR SUPPORT VECTOR MACHINE\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "lsvc_classifier = LinearSVC(labelCol = labelCol,\n",
    "                            featuresCol = featuresCol)\n",
    "# build specific pipeline\n",
    "lsvc_pipeline = build_full_pipeline(lsvc_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_lsvc = lsvc_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_lsvc = model_lsvc.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_lsvc = evaluator.evaluate(predictions_lsvc, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_lsvc = evaluator.evaluate(predictions_lsvc, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_lsvc, prec_lsvc, rec_lsvc, f1_lsvc = conf_metrics(predictions_lsvc)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"LSVC: Train set evaluation metrics\")\n",
    "display_metrics(predictions_lsvc, roc_lsvc, pr_lsvc)\n",
    "print(\"\")\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_lsvc, \"LSVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d5d37",
   "metadata": {},
   "source": [
    "### <font color='blue'>Choose the Best Classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebf49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Pandas dataframe with metrics\n",
    "dict_metrics = {\"LinReg\": [acc_lr, prec_lr, rec_lr, f1_lr, roc_lr, pr_lr],\n",
    "                \"DecTrees\": [acc_dt, prec_dt, rec_dt, f1_dt, roc_dt, pr_dt], \n",
    "                \"RandForest\": [acc_rf, prec_rf, rec_rf, f1_rf, roc_rf, pr_rf],\n",
    "                \"GradBoost\": [acc_gbt, prec_gbt, rec_gbt, f1_gbt, roc_gbt, pr_gbt],\n",
    "                \"MultiLPerceptron\": [acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc, roc_mlpc, pr_mlpc],\n",
    "                \"LinearSVM\": [acc_lsvc, prec_lsvc, rec_lsvc, f1_lsvc, roc_lsvc, pr_lsvc],\n",
    "                \"list_metrics\" : [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc_roc\", \"auc_pr\"]\n",
    "               }\n",
    "df_mets = pd.DataFrame.from_dict(dict_metrics).set_index(\"list_metrics\")\n",
    "df_mets.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f805cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid stacking the plots\n",
    "plt.clf()\n",
    "\n",
    "# adjust figure size and font size\n",
    "sns.set(rc = {\"figure.figsize\":(12,4)})\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "ax = df_mets.plot.bar(y=[\"LinReg\", \"DecTrees\", \n",
    "                         \"RandForest\", \"GradBoost\", \"MultiLPerceptron\", \"LinearSVM\"], rot=0)\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "\n",
    "# create title and labels\n",
    "plt.title(\"Confusion Matrix Metrics For Sparkify Small Train Dataset\")\n",
    "ax.set_xlabel(\"\");\n",
    "ax.set_ylabel(\"\");\n",
    "\n",
    "# display the plot\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701dee0",
   "metadata": {},
   "source": [
    "## <font color='blue'>Tune Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f433d8",
   "metadata": {},
   "source": [
    "### <font color='blue'>Gradient Boosted Trees</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Gradient Boosted Trees\"\n",
    "def gbt_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Gradient Boosted Trees\n",
    "    param_gbt = ParamGridBuilder()\n",
    "    param_gbt = param_gbt.addGrid(model.maxDepth, [10, 20, 30])\n",
    "    param_gbt = param_gbt.addGrid(model.maxIter, [10, 20, 40]) \n",
    "    param_gbt = param_gbt.addGrid(model.stepSize, [.05, .1])\n",
    "    param_gbt = param_gbt.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_gbt)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training GRADIENT BOOSTED TREES\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "gbt_classifier = GBTClassifier(labelCol = labelCol,\n",
    "                                featuresCol = featuresCol,\n",
    "                                seed=1234)\n",
    "# build specific pipeline\n",
    "gbt_pipeline = build_full_pipeline(gbt_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "gbt = gbt_grid_search(gbt_pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_gbt = gbt.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_gbt = model_gbt.bestModel.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_gbt, prec_gbt, rec_gbt, f1_gbt = conf_metrics(predictions_gbt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"GBT: Evaluation metrics on the test set\")\n",
    "display_metrics(predictions_gbt, roc_gbt, pr_gbt)\n",
    "print(\"\")\n",
    "\n",
    "# print the best parameters from the grid\n",
    "best_model_gbt = model_gbt.bestModel.stages[-1]\n",
    "param_gbt1 = best_model_gbt.getMaxDepth()\n",
    "param_gbt2 = best_model_gbt.getMaxIter()\n",
    "param_gbt3 = best_model_gbt.getStepSize()\n",
    "\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"maxDepth:..........\", param_gbt1)\n",
    "print(\"maxIter:...........\", param_gbt2)\n",
    "print(\"stepSize:..........\", param_gbt3)\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_gbt, \"GBT\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_gbt, \"GBT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62814e",
   "metadata": {},
   "source": [
    "### <font color='blue'>Multilayer Perceptron Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180777e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Multilayer Perceptron Classifier\"\n",
    "def mlpc_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Multilayer Perceptron Classifier\n",
    "    param_mlpc = ParamGridBuilder()\n",
    "    param_mlpc = param_mlpc.addGrid(model.stepSize, [.05, .1])\n",
    "    param_mlpc = param_mlpc.addGrid(model.maxIter, [100, 200]) \n",
    "    param_mlpc = param_mlpc.addGrid(model.blockSize, [64, 128, 256]) \n",
    "    param_mlpc = param_mlpc.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_mlpc)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training MULTILAYER PERCEPTRON CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "# specify layers: 18 (features), two intermediate (8, 4), output 2 (classes)\n",
    "layers=[18, 8, 4, 2]\n",
    "# create the trainer and set its parameters\n",
    "mlpc_classifier = MultilayerPerceptronClassifier(labelCol = labelCol,\n",
    "                                                featuresCol = featuresCol,\n",
    "                                                layers=layers,\n",
    "                                                seed=1234)\n",
    "# build specific pipeline\n",
    "mlpc_pipeline = build_full_pipeline(mlpc_classifier)\n",
    "\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "mlpc = mlpc_grid_search(mlpc_pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_mlpc = mlpc.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_mlpc = model_mlpc.bestModel.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc = conf_metrics(predictions_mlpc)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "display_metrics(predictions_mlpc, roc_mlpc, pr_mlpc)\n",
    "print(\"\")\n",
    "\n",
    "# print the best parameters from the grid\n",
    "best_model_mlpc = model_mlpc.bestModel.stages[-1]\n",
    "param_mlpc1 = best_model_mlpc.getStepSize()\n",
    "param_mlpc2 = best_model_mlpc.getMaxIter()\n",
    "param_mlpc3 = best_model_mlpc.getBlockSize()\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"stepSize:..........\", param_mlpc1)\n",
    "print(\"maxIter:...........\", param_mlpc2)\n",
    "print(\"blockSize:.........\", param_mlpc3)\n",
    "\n",
    "# plot heatmap for confusion matrix\n",
    "conf_mat_heatmap(predictions_lsvc, \"LSVC\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_mlpc, \"MLPC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18539ca0",
   "metadata": {},
   "source": [
    "### <font color='blue'>Compare Tuned Classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c099eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_pr_curves(predictions_model1, predictions_model2, model1, model2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plots the ROC and PR curves for two models on the same graphs.\n",
    "    \n",
    "    INPUT:\n",
    "        predictions_model1 (PySpark dataframe) - contains probability predictions for the first model\n",
    "        predictions_model2 (PySpark dataframe) - contains probability predictions for the second model\n",
    "        model1, model2 (str) - abreviated model names\n",
    "        \n",
    "    OUTPUT:\n",
    "        none - two plots are displayed\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # transform predictions PySpark dataframe into Pandas dataframe\n",
    "    pred1_pandas = predictions_model1.select(predictions_model1.label, \n",
    "                                             predictions_model1.probability).toPandas()\n",
    "    pred2_pandas = predictions_model2.select(predictions_model2.label, \n",
    "                                             predictions_model2.probability).toPandas()\n",
    "    \n",
    "    # calculate roc_auc scores\n",
    "    roc_auc1 = roc_auc_score(pred1_pandas.label, pred1_pandas.probability.str[1])\n",
    "    # calculate roc_auc scores\n",
    "    roc_auc2 = roc_auc_score(pred2_pandas.label, pred2_pandas.probability.str[1])\n",
    "    \n",
    "    # calculate roc curves for model 1\n",
    "    fpr1, tpr1, _ = roc_curve(pred1_pandas.label, pred1_pandas.probability.str[1])\n",
    "    # calculate roc curves for model 2\n",
    "    fpr2, tpr2, _ = roc_curve(pred2_pandas.label, pred2_pandas.probability.str[1])\n",
    "   \n",
    "    # calculate precision, recall for each threshold for the first model\n",
    "    precision1, recall1, _ = precision_recall_curve(pred1_pandas.label, pred1_pandas.probability.str[1])\n",
    "    # calculate pr auc score for the first model\n",
    "    pr_auc1 = auc(recall1, precision1)\n",
    "    \n",
    "    # calculate precision, recall for each threshold for the second model\n",
    "    precision2, recall2, _ = precision_recall_curve(pred2_pandas.label, pred2_pandas.probability.str[1])\n",
    "    # calculate pr auc score for the second model\n",
    "    pr_auc2 = auc(recall2, precision2)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    # create figure which contains two subplots\n",
    "    plt.figure(figsize=[12,6])\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    \n",
    "    # plot the roc curve for the model1\n",
    "    plt.plot(ns_fpr1, ns_tpr1, linestyle='--', label='No Skill')\n",
    "    plt.plot(fpr1, tpr1, marker='.', color='firebrick', label=model1 + 'ROC AUC = %.3f' % (roc_auc1))\n",
    "    plt.plot(fpr2, tpr2, marker='.', color='green', label=model2 + 'ROC AUC = %.3f' % (roc_auc2))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"ROC Curves\")\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    # plot the precision-recall curves\n",
    "    \n",
    "    #ns_line = len(pred_pandas[pred_pandas.label==1]) / len(pred_pandas.label)\n",
    "    #plt.plot([0, 1], [ns_line, ns_line], linestyle='--', label='No Skill')\n",
    "    plt.plot(recall1, precision1, marker='.', color=\"firebrick\", label=model1+ 'PR AUC = %.3f' % (pr_auc1))\n",
    "    plt.plot(recall2, precision2, marker='.', color=\"green\", label=model2+'PR AUC = %.3f' % (pr_auc2))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"Precision-Recall Curves\")\n",
    "\n",
    "    # display the plot\n",
    "    %matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the two pairs of curves for comparison\n",
    "plot_roc_pr_curves(predictions_mlpc, predictions_gbt, \"MLPC\", \"GBT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4607cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Pandas dataframe with evaluation metrics for the best hyperparameters selection\n",
    "dict_metrics = {\"GradBoost\": [acc_gbt, prec_gbt, rec_gbt, f1_gbt, roc_gbt, pr_gbt],\n",
    "                \"MultiLPerceptron\": [acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc, roc_mlpc, pr_mlpc],\n",
    "                \"list_metrics\" : [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc_roc\", \"auc_pr\"]\n",
    "               }\n",
    "df_mets = pd.DataFrame.from_dict(dict_metrics).set_index(\"list_metrics\")\n",
    "df_mets.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f28b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot of the evaluation metrics for the two models, for comparison\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# adjust figure size and font size\n",
    "sns.set(rc = {\"figure.figsize\":(12,2)})\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "ax = df_mets.plot.bar(y=[\"GradBoost\", \"MultiLayerPerceptron\"], rot=0)\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")_\n",
    "\n",
    "# create title and labels\n",
    "plt.title(\"Evaluation Metrics For Sparkify Full Test Dataset\")\n",
    "ax.set_xlabel(\"\");\n",
    "ax.set_ylabel(\"\");\n",
    "\n",
    "# display the plot\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429af780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
