{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\": {\"spark.pyspark.python\":\"/home/hadoop/path/to/venv/bin/python3\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a7375",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> Sparkify Stacking Models on Full Dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b9c81",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This notebook is dedicated to the task of creating a stacking model based on individual classifiers available in PySpark. The model is fit and evaluated on subsets of the full Sparkify dataset.\n",
    "    <li> clean and preprocess the data</li>\n",
    "    <li> split the data in train, validation and test sets</li>\n",
    "    <li> train each of the base classifiers on the train set</li>\n",
    "    <li> have each base classifier make predictions on the validation set</li>\n",
    "    <li> create a meta-features dataset from the individual predictions</li>\n",
    "    <li> train a linear regression classifier on the meta-features dataset</li>\n",
    "    <li> evaluate the meta classifier on the test set</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40a332",
   "metadata": {},
   "source": [
    "## <font color='blue'>Build the Workspace</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PySpark libraries and packages\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    IntegerType, \n",
    "    DateType, \n",
    "    TimestampType,\n",
    "    )\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    min as Fmin, max as Fmax, \n",
    "    sum as Fsum, round as Fround, \n",
    "    \n",
    "    col, lit, \n",
    "    first, last, \n",
    "    desc, asc,\n",
    "    avg, count, countDistinct, \n",
    "    when, isnull, isnan,\n",
    "    from_unixtime, \n",
    "    datediff,\n",
    "    )\n",
    "\n",
    "# libraries and packages for modeling\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder, \n",
    "    OneHotEncoderModel\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier,\n",
    "    MultilayerPerceptronClassifier,\n",
    "    LinearSVC\n",
    ")\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a Spark session using the SparkSession APIs\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Sparkify\")\n",
    "        .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0794133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import sklearn metrics related packages\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "\n",
    "# import library for enhanced plotting\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "colors = sns.color_palette('PuBuGn_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee5859",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Load, Preprocess and Split Data Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the raw dataset in Spark.\n",
    "    \n",
    "    INPUT:\n",
    "            (str) - path for datafile\n",
    "    OUTPUT:\n",
    "            (PySpark dataframe) - dataframe of raw data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading the dataset ...\")\n",
    "    df = spark.read.json(file_path)\n",
    "    print(\"Dataset is loaded...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Performs basic cleaning operations on the raw data:\n",
    "        - removes entries with missing userId\n",
    "        - rescale timestamp columns to seconds\n",
    "        - drop unnecesary columns\n",
    "            - personal information columns\n",
    "            - song information columns\n",
    "            - web and browser information\n",
    "            - timestamp columns in miliseconds\n",
    "\n",
    "    INPUT:\n",
    "        (PySpark dataframe) - dataframe of raw data\n",
    "    OUTPUT:\n",
    "        (PySpark dataframe) - dataframe of cleaned data\n",
    "    \"\"\"\n",
    "\n",
    "    # print message to indicate the start of the process\n",
    "    print(\"Cleaning the data ...\")\n",
    "\n",
    "    # print a count of rows before cleaning\n",
    "    initial_records = df.count()\n",
    "    print(\"Dataset has {} rows initially.\".format(initial_records))\n",
    "\n",
    "    # remove all the records without userId\n",
    "    df = df.where(df.userId != \"\")\n",
    "\n",
    "    # rescale the timestamp to seconds (initially in miliseconds)\n",
    "    df = df.withColumn(\"log_ts\", df.ts/1000.0)\n",
    "    df = df.withColumn(\"reg_ts\", df.registration/1000.0)\n",
    "\n",
    "    # drop several unnecessary columns\n",
    "    cols_to_drop = (\"firstName\", \"lastName\", \"location\",\n",
    "                    \"artist\", \"song\", \"length\",\n",
    "                    \"userAgent\", \"method\", \"status\",\n",
    "                    \"ts\", \"registration\"\n",
    "                   )\n",
    "    df = df.drop(*cols_to_drop)\n",
    "\n",
    "\n",
    "    # print end of process message\n",
    "    print(\"Finished cleaning the data ...\")\n",
    "\n",
    "    # print a count of rows after cleaning\n",
    "    removed_rows = initial_records - df.count()\n",
    "\n",
    "    print(\"Cleaned dataset has {} rows, {} rows were removed\". \\\n",
    "        format(df.count(), initial_records - df.count()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare the data for modeling via creating several features.\n",
    "\n",
    "        - reg_date (date) - month-year of the registration\n",
    "\n",
    "        - create windows grouped on userId and sessionId\n",
    "\n",
    "         - firstevent_ts (timestamp) - first time an user is active\n",
    "         - lastevent_ts (timestamp) - last time an user is active\n",
    "\n",
    "         - init_days_interv (float) - days between registration and first activity\n",
    "         - tenure_days_interv (float) - days between registration and last activity\n",
    "         - active_days (float) - days the user has some activity on the platform\n",
    "         - session_h (float) - session's duration in hours\n",
    "\n",
    "     INPUT:\n",
    "         df (PySpark dataframe) - cleaned dataframe\n",
    "     OUTPUT:\n",
    "         df (PySpark dataframe) - dataframe with the listed features added\n",
    "    \"\"\"\n",
    "\n",
    "    # extract registration month and year from timestamp\n",
    "    df = df.withColumn(\"reg_date\", from_unixtime(col(\"reg_ts\"), \"MM-yyyy\"))\n",
    "\n",
    "    # create window: data grouped by userId, time ordered\n",
    "    win_user = (W.partitionBy(\"userId\")\n",
    "            .orderBy(\"log_ts\")\n",
    "            .rangeBetween(W.unboundedPreceding,\n",
    "                          W.unboundedFollowing))\n",
    "\n",
    "    # create window: data grouped by sessionId and userId, time ordered\n",
    "    win_user_session = (W.partitionBy(\"sessionId\", \"userId\")\n",
    "                        .orderBy(\"log_ts\")\n",
    "                        .rangeBetween(W.unboundedPreceding,\n",
    "                                      W.unboundedFollowing))\n",
    "\n",
    "    # record the first time an user is active\n",
    "    df = df.withColumn(\"firstevent_ts\", first(col(\"log_ts\")).over(win_user))\n",
    "    # record the last time an user is active\n",
    "    df = df.withColumn(\"lastevent_ts\", last(col(\"log_ts\")).over(win_user))\n",
    "\n",
    "    # warmup time = registration time to first event in days\n",
    "    df = df.withColumn(\"init_days_interv\",\n",
    "                       (col(\"firstevent_ts\").cast(\"long\")-col(\"reg_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # tenure time = registration time to last event in days\n",
    "    df = df.withColumn(\"tenure_days_interv\",\n",
    "                       (col(\"lastevent_ts\").cast(\"long\")-col(\"reg_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # active time =  days between the first event and the last event in days\n",
    "    df = df.withColumn(\"active_days\",\n",
    "                       (col(\"lastevent_ts\").cast(\"long\")-col(\"firstevent_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # create column that records the individual session's duration in hours\n",
    "    df = df.withColumn(\"session_h\",\n",
    "                    (last(df.log_ts).over(win_user_session) \\\n",
    "                     - first(df.log_ts).over(win_user_session))/3600)\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(\"reg_ts\", \"log_ts\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_features(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Features engineered to be used in modelling.\n",
    "\n",
    "        - nr_songs (int) - total number of songs user listened to\n",
    "        - nr_playlist (int) - number of songs added to the playlist\n",
    "\n",
    "        - nr_friends (int) - number of friends added through \"Add Friend\"\n",
    "\n",
    "        - nr_likes (int) - total number of \"Thumbs Up\" of the user\n",
    "        - nr_dislikes (int) - total number of \"Thumbs Down\" of the user\n",
    "\n",
    "        - nr_downgrades (int) - total number of visits to \"Downgrade\" page by the user\n",
    "        - nr_upgrades (int) - total number of visits to \"Upgrade\" page by the user\n",
    "\n",
    "        - nr_home (int) - total number of visits to \"Home\" page by the user\n",
    "        - nr_settings (int) - total number of visits to \"Settings\" page by the user\n",
    "\n",
    "        - nr_error (int) - total number of errors encountered by the user\n",
    "\n",
    "        - nr_ads (int) - total number of ads the user got\n",
    "        - nr_sessions (int) - number of sessions of the user\n",
    "        - n_acts (int) - total number of actions taken by the user\n",
    "\n",
    "        - avg_sess_h (float) - average session length in hours\n",
    "        - acts_per_session (float) - average number of actions per session for the user\n",
    "        - songs_per_session (float) - average numer of songs listened per session by the user\n",
    "        - ads_per_session (float) - average number of ads per session, received by user\n",
    "\n",
    "        - init_days_interv (int) - time interval in days from registration to the first action of the user\n",
    "        - tenure_days_interv (int) - time interval in days from registration to the last action of the user\n",
    "        - active_days (int) - number of days the user was active on the platform\n",
    "\n",
    "        - gender (binary) - 1 for F (female), 0 for M (male)\n",
    "        - level (binary) - 1 for paid, 0 for free\n",
    "\n",
    "        - churn (binary) - 1 for \"Cancellation Confirmation\" page visit, 0 otherwise\n",
    "\n",
    "    INPUT:\n",
    "        df (PySpark dataframe) - preprocessed dataframe\n",
    "    OUTPUT:\n",
    "        df_feats (PySpark dataframe) - dataframe that contains engineered features\n",
    "    \"\"\"\n",
    "\n",
    "    df_feats = df.groupBy(\"userId\") \\\n",
    "        .agg(\n",
    "\n",
    "            # count user's individual actions using all page visits\n",
    "\n",
    "            count(when(col(\"page\") == \"NextSong\", True)).alias(\"nr_songs\"),\n",
    "            count(when(col(\"page\") == \"Add to Playlist\", True)).alias(\"nr_playlist\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Add Friend\", True)).alias(\"nr_friends\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Thumbs Up\", True)).alias(\"nr_likes\"),                count(when(col(\"page\") == \"Thumbs Down\", True)).alias(\"nr_dislikes\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Downgrade\", True)).alias(\"nr_downgrades\"),\n",
    "            count(when(col(\"page\") == \"Upgrade\", True)).alias(\"nr_upgrades\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Home\", True)).alias(\"nr_home\"),\n",
    "            count(when(col(\"page\") == \"Settings\", True)).alias(\"nr_settings\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Error\", True)).alias(\"nr_error\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Roll Advert\", True)).alias(\"nr_ads\"),\n",
    "\n",
    "            # compute the number of sessions a user is in\n",
    "            countDistinct(\"sessionId\").alias(\"nr_sessions\"),\n",
    "\n",
    "            # find the total number of actions a user took\n",
    "            countDistinct(\"itemInSession\").alias(\"n_acts\"),\n",
    "\n",
    "            # compute the average session length in hours\n",
    "            avg(col(\"session_h\")).alias(\"avg_sess_h\"),\n",
    "\n",
    "            # compute the average number of page actions per sesssion - i.e. items in session\n",
    "            (countDistinct(\"itemInSession\") /countDistinct(\"sessionId\")).alias(\"acts_per_session\"),\n",
    "\n",
    "            # compute the average number of songs per session\n",
    "            (count(when(col(\"page\") == \"NextSong\", True)) /countDistinct(\"sessionId\")).alias(\"songs_per_session\"),\n",
    "\n",
    "            # compute the average number of ads per session\n",
    "             (count(when(col(\"page\") == \"Roll Advert\", True)) /countDistinct(\"sessionId\")).alias(\"ads_per_session\"),\n",
    "\n",
    "            # days between registration and first activity\n",
    "            first(col(\"init_days_interv\")).alias(\"init_days_interv\"),\n",
    "            # the tenure time on the platform: from registration to last event in days\n",
    "            first(col(\"tenure_days_interv\")).alias(\"tenure_days_interv\"),\n",
    "            # number of days user visited the platform, is active on the platform\n",
    "            first(col(\"active_days\")).alias(\"active_days\"),\n",
    "\n",
    "            # encode the gender 1 for F and 0 for M\n",
    "            first(when(col(\"gender\") == \"F\", 1).otherwise(0)).alias(\"gender\"),\n",
    "\n",
    "            # encode the level (paid/free) according to the last record\n",
    "            last(when(col(\"level\") == \"paid\", 1).otherwise(0)).alias(\"level\"),\n",
    "\n",
    "            # flag those users that downgraded\n",
    "            #last(when(col(\"page\") == \"Downgrade\", 1).otherwise(0)).alias(\"downgrade\"),\n",
    "\n",
    "            # create the churn column that records if the user cancelled\n",
    "            last(when(col(\"page\") == \"Cancellation Confirmation\", 1).otherwise(0)).alias(\"churn\"),\n",
    "            )\n",
    "\n",
    "    # columns to drop\n",
    "    drop_cols = (\"userId\", \"gender\", \"avg_sess_h\",\n",
    "                 \"nr_playlist\", \"nr_home\")\n",
    "    # drop the columns\n",
    "    #df_feats = df_feats.drop(\"userId\")\n",
    "    df_feats = df_feats.drop(*drop_cols)\n",
    "\n",
    "    # drop the null values\n",
    "    df_feats=df_feats.na.drop()\n",
    "\n",
    "    return df_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da41c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data (df):\n",
    "\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation set and test set.\n",
    "    Use a stratified sampling method.\n",
    "\n",
    "    INPUT:\n",
    "        df (PySpark dataframe) - dataframe\n",
    "    OUTPUT:\n",
    "        train_set, validation_set, test_set (PySpark dataframes) - \n",
    "                          percentage split based on the provided values\n",
    "    \"\"\"\n",
    "\n",
    "    # split dataframes between 0s and 1s\n",
    "    zeros = df.filter(df[\"churn\"]==0)\n",
    "    ones = df.filter(df[\"churn\"]==1)\n",
    "\n",
    "    # split dataframes into training and testing\n",
    "    train0, validation0, test0 = zeros.randomSplit(SPLIT_VALS, seed=1234)\n",
    "    train1, validation1, test1 = ones.randomSplit(SPLIT_VALS, seed=1234)\n",
    "\n",
    "    # stack datasets back together\n",
    "    train_set = train0.union(train1)\n",
    "    validation_set = validation0.union(validation1)\n",
    "    test_set = test0.union(test1)\n",
    "\n",
    "    return train_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_filepath):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that combines all the data preparation steps.\n",
    "    \n",
    "    INPUT:\n",
    "        dataset_filepath (str) - filepath for raw data json file\n",
    "    OUTPUT:\n",
    "       train_set, validation_set, test_set (PySpark dataframes) - \n",
    "            subsets of the features dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Load data...')\n",
    "    df = load_data(dataset_filepath)\n",
    "    \n",
    "    print('Clean data...')\n",
    "    df_clean = clean_data(df)\n",
    "    \n",
    "    print('Preprocess data...')\n",
    "    df_proc = preprocess_data(df_clean)\n",
    "    \n",
    "    print('Create features dataset...')\n",
    "    df_feats = build_features(df_proc)\n",
    "    \n",
    "    print('Split the features dataset...')\n",
    "    train_set, validation_set, test_set = split_data(df_feats)\n",
    "    \n",
    "    return train_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26229e06",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Model Evaluators Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b88f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute relevant metrics for binary classification\n",
    "def conf_metrics(dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "        Calculates the metrics associated to the confusion matrix.\n",
    "\n",
    "        INPUT:\n",
    "            dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "        OUTPUT:\n",
    "            accuracy (float) - metric\n",
    "            precision (float) - metric\n",
    "            recall (float) - metric\n",
    "            F1 (float) - metric\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # calculate the elements of the confusion matrix\n",
    "    tn = dataset.where((dataset[meta_labelCol]==0) & (dataset[meta_predCol]==0)).count()\n",
    "    tp = dataset.where((dataset[meta_labelCol]==1) & (dataset[meta_predCol]==1)).count()                   \n",
    "    fn = dataset.where((dataset[meta_labelCol]==1) & (dataset[meta_predCol]==0)).count()                   \n",
    "    fp = dataset.where((dataset[meta_labelCol]==0) & (dataset[meta_predCol]==1)).count()\n",
    "    \n",
    "    # calculate accuracy, precision, recall, and F1-score\n",
    "    accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 =  2 * (precision*recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# function to display the metrics of interest\n",
    "def display_metrics(dataset, roc_cl, pr_cl):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints evaluation metrics for the model. \n",
    "    \n",
    "    INPUT:\n",
    "         dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "        roc_cl, pr_cl (float) - ROC and PR scores for the evaluate classifier cl\n",
    "    OUTPUT:\n",
    "        none - table of metrics are displayed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = conf_metrics(dataset)[0]\n",
    "    precision = conf_metrics(dataset)[1]\n",
    "    recall = conf_metrics(dataset)[2]\n",
    "    f1 = conf_metrics(dataset)[3]\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    dataset.groupBy(dataset[meta_labelCol], dataset[meta_predCol]).count().show()\n",
    "    print(\"\")\n",
    "    print(\"accuracy...............%6.3f\" % accuracy)\n",
    "    print(\"precision..............%6.3f\" % precision)\n",
    "    print(\"recall.................%6.3f\" % recall)\n",
    "    print(\"F1.....................%6.3f\" % f1)\n",
    "    print(\"auc_roc................%6.3f\" % roc_cl)\n",
    "    print(\"auc_pr.................%6.3f\" % pr_cl)\n",
    "\n",
    "# function to plot the roc and pr curves side by side (uses Pandas)\n",
    "def plot_roc_pr_curves(predictions, model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates ROC-AUC and PR-AUC scores and plots the ROC and PR curves.\n",
    "    \n",
    "    INPUT:\n",
    "        predictions (PySpark dataframe) - contains probability predictions, label column\n",
    "        model_name (str) - classifier name\n",
    "        \n",
    "    OUTPUT:\n",
    "        none - two plots are displayed\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # transform predictions PySpark dataframe into Pandas dataframe\n",
    "    pred_pandas = predictions.select(predictions.label, predictions.probability).toPandas()\n",
    "    \n",
    "    # calculate roc_auc score\n",
    "    roc_auc = roc_auc_score(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(pred_pandas.label))]\n",
    "    # calculate roc curves\n",
    "    fpr, tpr, _ = roc_curve(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(pred_pandas.label, ns_probs)\n",
    "    \n",
    "    # calculate precision, recall for each threshold\n",
    "    precision, recall, _ = precision_recall_curve(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    # calculate pr auc score\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # create figure which contains two subplots\n",
    "    plt.figure(figsize=[12,6])\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    \n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    plt.plot(fpr, tpr, marker='.', color=\"firebrick\", label='ROC AUC = %.3f' % (roc_auc))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"ROC Curve:\" + model_name)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    # plot the precision-recall curves\n",
    "    \n",
    "    ns_line = len(pred_pandas[pred_pandas.label==1]) / len(pred_pandas.label)\n",
    "    plt.plot([0, 1], [ns_line, ns_line], linestyle='--', label='No Skill')\n",
    "    plt.plot(recall, precision, marker='.', color=\"firebrick\", label='PR AUC = %.3f' % (pr_auc))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"Precision-Recall Curve:\" + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98db8e",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Build Pipelines and Instantiate Models</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base models\n",
    "# use different labels for the prediction columns to avoid overlap when stacking\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', \n",
    "                        predictionCol='pred_lr', probabilityCol='prob_lr',\n",
    "                        rawPredictionCol='rawPred_lr')\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', \n",
    "                  predictionCol='pred_rf', probabilityCol='prob_rf', \n",
    "                  rawPredictionCol='rawPred_rf')\n",
    "\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='label',\n",
    "                    predictionCol='pred_gbt')\n",
    "\n",
    "layers=[18,8,4,2]\n",
    "mlpc= MultilayerPerceptronClassifier(featuresCol='features', labelCol='label',\n",
    "                                     predictionCol='pred_mlpc', probabilityCol='prob_mlpc',\n",
    "                                     rawPredictionCol='rawPred_mlpc', layers=layers)\n",
    "\n",
    "lsvc = LinearSVC(featuresCol='features', labelCol='label', \n",
    "                predictionCol='pred_lsvc', rawPredictionCol='rawPred_lsvc')\n",
    "\n",
    "models = [lr, rf, gbt, mlpc, lsvc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the features and the label\n",
    "CAT_FEATURES = [\"level\"]\n",
    "CONT_FEATURES = [\"nr_songs\", \"nr_likes\", \"nr_dislikes\", \"nr_friends\", \"nr_downgrades\",\n",
    "                \"nr_upgrades\", \"nr_error\", \"nr_settings\", \"nr_ads\", \"nr_sessions\",\n",
    "                \"n_acts\", \"acts_per_session\", \"songs_per_session\", \"ads_per_session\",\n",
    "                \"init_days_interv\", \"tenure_days_interv\", \"active_days\"]\n",
    "CHURN_LABEL = \"churn\"\n",
    "\n",
    "def build_data_pipeline():\n",
    "    \"\"\"\n",
    "    Combines all the stages of the data processing.\n",
    "    \"\"\"\n",
    "    # stages in the pipeline\n",
    "    stages = [] \n",
    "    \n",
    "    # encode the labels\n",
    "    label_indexer =  StringIndexer(inputCol=CHURN_LABEL, outputCol=\"label\")\n",
    "    stages += [label_indexer]\n",
    "    \n",
    "    # encode the binary features\n",
    "    bin_assembler = VectorAssembler(inputCols=CAT_FEATURES, outputCol=\"bin_features\")\n",
    "    stages += [bin_assembler]\n",
    "    \n",
    "    # encode the continuous features\n",
    "    cont_assembler = VectorAssembler(inputCols = CONT_FEATURES, outputCol=\"cont_features\")\n",
    "    stages += [cont_assembler]\n",
    "    # normalize the continuous features\n",
    "    cont_scaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaler\", \n",
    "                                 withStd=True , withMean=True)\n",
    "    stages += [cont_scaler]\n",
    "    \n",
    "    # pass all to the vector assembler to create a single sparse vector\n",
    "    all_assembler = VectorAssembler(inputCols=[\"bin_features\", \"cont_scaler\"],  \n",
    "                            outputCol=\"features\")\n",
    "    stages += [all_assembler]\n",
    "    \n",
    "    # add the models to the pipeline\n",
    "    stages += models\n",
    "    \n",
    "    # create a pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b068d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the meta_features for modeling\n",
    "\n",
    "# split the features and the label\n",
    "META_FEATURES = [\"pred_lr\", \"pred_rf\",\"pred_gbt\", \"pred_mlpc\", \"pred_lsvc\"]\n",
    "META_CONT_FEATURES = [\"prob_lr\", \"prob_rf\", \"prob_mlpc\"]\n",
    "META_LABEL_COL = \"label\"\n",
    "\n",
    "# create labels and features\n",
    "meta_predCol=\"meta_prediction\"\n",
    "meta_labelCol=\"meta_label\"\n",
    "meta_featuresCol = \"meta_features\"\n",
    "\n",
    "def build_meta_pipeline(meta_classifier):\n",
    "    \"\"\"\n",
    "    Combines all the stages of the meta features processing.\n",
    "    \"\"\"\n",
    "    # stages in the pipeline\n",
    "    stages = [] \n",
    "    \n",
    "    # encode the labels\n",
    "    label_indexer =  StringIndexer(inputCol=META_LABEL_COL, outputCol=\"meta_label\")\n",
    "    stages += [label_indexer]\n",
    "    \n",
    "    # encode the binary features\n",
    "    bin_assembler = VectorAssembler(inputCols=META_FEATURES, outputCol=\"bin_features\")\n",
    "    stages += [bin_assembler]\n",
    "    \n",
    "    # encode the continuous features\n",
    "    cont_assembler = VectorAssembler(inputCols = META_CONT_FEATURES, outputCol=\"cont_features\")\n",
    "    stages += [cont_assembler]\n",
    "    # normalize the continuous features\n",
    "    cont_scaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaler\", \n",
    "                                 withStd=True , withMean=True)\n",
    "    stages += [cont_scaler]\n",
    "    \n",
    "    # pass all to the vector assembler to create a single sparse vector\n",
    "    all_assembler = VectorAssembler(inputCols=[\"bin_features\", \"cont_scaler\"],  \n",
    "                                    outputCol=\"meta_features\")\n",
    "    stages += [all_assembler]\n",
    "    \n",
    "    # add the models to the pipeline\n",
    "    stages += [meta_classifier]\n",
    "    \n",
    "    # create a pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# implement K-fold cross validation and grid search \n",
    "def grid_search_model(pipeline, paramGrid):\n",
    "    \"\"\"\n",
    "    Creates a cross validation object and performs grid search\n",
    "    over a set of parameters.\n",
    "    \n",
    "    INPUT:\n",
    "        param = grid of parameters\n",
    "        pipeline = model pipeline \n",
    "    \n",
    "    OUTPUT:\n",
    "        cv = cross validation object\n",
    "    \"\"\"\n",
    "    \n",
    "    # choose an evaluator\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    evaluator.setLabelCol(meta_labelCol)\n",
    "    \n",
    "    # create the cross-validation object\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5,\n",
    "                    parallelism=2)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd8447",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Process and Model Base Classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALS = [.4, .3, .3]   \n",
    "\n",
    "# path for the dataset file\n",
    "path_dataset = \"data/mini_sparkify_event_data.json\"\n",
    "#path_dataset = \"data/full_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa710a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean, process and split the data\n",
    "train_df, validation_df, test_df = prepare_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data in the memory and disk\n",
    "train_cached = train_df.persist()\n",
    "validation_cached = validation_df.persist()\n",
    "test_cached = test_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the data pipeline to process data\n",
    "base_pipeline = build_data_pipeline()\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# fit the pipeline on the transformed train set\n",
    "base_pipeline_model = base_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time \n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4823b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the validation set\n",
    "base_pred = base_pipeline_model.transform(validation_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the base_pred dataframe structure\n",
    "base_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84a3a0",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Toggle the Memory</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the raw datasets from memory\n",
    "train_cached.unpersist()\n",
    "validation_cached.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29bc5c",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">The Metaclassifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the meta features dataset\n",
    "meta_features_df = base_pred.select(\"pred_lr\", \"pred_rf\", \"pred_gbt\", \n",
    "                                    \"pred_mlpc\",\"pred_lsvc\",\n",
    "                                    \"prob_lr\", \"prob_rf\", \"prob_mlpc\",\n",
    "                                    \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training LOGISTIC REGRESSION META CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "meta_classifier = LogisticRegression(featuresCol=meta_featuresCol, \n",
    "                             labelCol=meta_labelCol, \n",
    "                             predictionCol=meta_predCol)\n",
    "\n",
    "# build specific pipeline\n",
    "meta_pipeline = build_meta_pipeline(meta_classifier)\n",
    "\n",
    "# create the parameters list\n",
    "meta_param = ParamGridBuilder() \\\n",
    "        .addGrid(meta_classifier.regParam, [0.0, 0.01, 0.1]) \\\n",
    "        .addGrid(meta_classifier.elasticNetParam, [0.0, 0.01, 0.1]) \\\n",
    "        .addGrid(meta_classifier.maxIter, [50, 100, 200]) \\\n",
    "        .build()\n",
    "\n",
    "\n",
    "# build the grid search model\n",
    "meta_lr = grid_search_model(meta_pipeline, meta_param)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "meta_model = meta_lr.fit(meta_features_df.persist())\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classifiers make predictions on the test set\n",
    "test_pred = base_pipeline_model.transform(test_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe772c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the meta features test dataset\n",
    "meta_test_df = test_pred.select(\"pred_lr\", \"pred_rf\", \"pred_gbt\", \n",
    "                                    \"pred_mlpc\",\"pred_lsvc\",\n",
    "                                    \"prob_lr\", \"prob_rf\", \"prob_mlpc\", \"prob_gbt\",\n",
    "                                    \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7834bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta classifier predictions on the meta features test set\n",
    "meta_test_pred = meta_model.transform(meta_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(meta_labelCol)\n",
    "\n",
    "# calculate roc and pr scores\n",
    "roc_meta = evaluator.evaluate(meta_test_pred, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_meta = evaluator.evaluate(meta_test_pred, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics on the test set\n",
    "acc_meta, prec_meta, rec_meta, f1_meta = conf_metrics(meta_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "display_metrics(meta_test_pred, roc_meta, pr_meta)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best parameters from the grid\n",
    "best_meta_param = list(meta_model.getEstimatorParamMaps()[np.argmax(meta_model.avgMetrics)].values())\n",
    "meta_param1 = best_meta_param[0]\n",
    "meta_param2 = best_meta_param[1]\n",
    "meta_param3 = best_meta_param[2]\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"regParam:..................\", meta_param1)\n",
    "print(\"elasticNetParam:...........\", meta_param2)\n",
    "print(\"maxIter:...................\", meta_param3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(meta_test_pred, \"Meta_LR\")\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf9508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
