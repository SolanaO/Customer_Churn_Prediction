{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47a7375",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> Sparkify Stacking Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b9c81",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This notebook is dedicated to the task of creating a stacking model based on individual classifiers available in PySpark. \n",
    "    <li> split the data in train, validation and test sets</li>\n",
    "    <li> train each of the classifiers on the train set</li>\n",
    "    <li> have each classifier make predictions on the validation set</li>\n",
    "    <li> create a meta-features dataset from the individual predictions</li>\n",
    "    <li> train a linear regression classifier on the meta-features dataset</li>\n",
    "    <li> evaluate the full model on the test set</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40a332",
   "metadata": {},
   "source": [
    "## <font color='blue'>Set Environment</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2a358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PySpark libraries and packages\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    IntegerType, \n",
    "    DateType, \n",
    "    TimestampType,\n",
    "    )\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    min as Fmin, max as Fmax, \n",
    "    sum as Fsum, round as Fround, \n",
    "    \n",
    "    col, lit, \n",
    "    first, last, \n",
    "    desc, asc,\n",
    "    avg, count, countDistinct, \n",
    "    when, isnull, isnan,\n",
    "    from_unixtime, \n",
    "    datediff,\n",
    "    )\n",
    "\n",
    "# libraries and packages for modeling\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder, \n",
    "    OneHotEncoderModel\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier,\n",
    "    MultilayerPerceptronClassifier,\n",
    "    LinearSVC,\n",
    "    NaiveBayes\n",
    ")\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38be1286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/30 22:11:28 WARN Utils: Your hostname, greg resolves to a loopback address: 127.0.1.1; using 192.168.0.21 instead (on interface wlp82s0)\n",
      "22/01/30 22:11:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/silvia/anaconda3/envs/pysparkenv/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/30 22:11:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# build a Spark session using the SparkSession APIs\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Sparkify\")\n",
    "        .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0794133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878c267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for enhanced plotting\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "colors = sns.color_palette('PuBuGn_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee5859",
   "metadata": {},
   "source": [
    "## <font color='blue'>Load Train and Test Datasets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c6df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the raw dataset in Spark.\n",
    "    \n",
    "    INPUT:\n",
    "            (str) - path for datafile\n",
    "    OUTPUT:\n",
    "            (PySpark dataframe) - dataframe of raw data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading the dataset ...\")\n",
    "    df = spark.read.json(file_path)\n",
    "    print(\"Dataset is loaded...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_data(df, data_path):\n",
    "    \"\"\"\n",
    "    Saves the PySpark dataframe to a file.\n",
    "    \n",
    "    INPUT:\n",
    "            df (PySpark dataframe) - data to be saved\n",
    "            data_path (str) - path for datafile\n",
    "    OUTPUT:\n",
    "            none\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df.write.json(data_path)\n",
    "    \n",
    "    \n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Performs basic cleaning operations on the raw data:\n",
    "        - removes entries with missing userId\n",
    "        - rescale timestamp columns to seconds\n",
    "        - drop unnecesary columns\n",
    "            - personal information columns\n",
    "            - song information columns\n",
    "            - web and browser information\n",
    "            - timestamp columns in miliseconds\n",
    "\n",
    "    INPUT:\n",
    "        (PySpark dataframe) - dataframe of raw data\n",
    "    OUTPUT:\n",
    "        (PySpark dataframe) - dataframe of cleaned data\n",
    "    \"\"\"\n",
    "\n",
    "    # print message to indicate the start of the process\n",
    "    print(\"Cleaning the data ...\")\n",
    "\n",
    "    # print a count of rows before cleaning\n",
    "    initial_records = df.count()\n",
    "    print(\"Dataset has {} rows initially.\".format(initial_records))\n",
    "\n",
    "    # remove all the records without userId\n",
    "    df = df.where(df.userId != \"\")\n",
    "\n",
    "    # rescale the timestamp to seconds (initially in miliseconds)\n",
    "    df = df.withColumn(\"log_ts\", df.ts/1000.0)\n",
    "    df = df.withColumn(\"reg_ts\", df.registration/1000.0)\n",
    "\n",
    "    # drop several unnecessary columns\n",
    "    cols_to_drop = (\"firstName\", \"lastName\", \"location\",\n",
    "                    \"artist\", \"song\", \"length\",\n",
    "                    \"userAgent\", \"method\", \"status\",\n",
    "                    \"ts\", \"registration\"\n",
    "                   )\n",
    "    df = df.drop(*cols_to_drop)\n",
    "\n",
    "\n",
    "    # print end of process message\n",
    "    print(\"Finished cleaning the data ...\")\n",
    "\n",
    "    # print a count of rows after cleaning\n",
    "    removed_rows = initial_records - df.count()\n",
    "\n",
    "    print(\"Cleaned dataset has {} rows, {} rows were removed\". \\\n",
    "        format(df.count(), initial_records - df.count()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare the data for modeling via creating several features.\n",
    "\n",
    "        - reg_date (date) - month-year of the registration\n",
    "\n",
    "        - create windows grouped on userId and sessionId\n",
    "\n",
    "         - firstevent_ts (timestamp) - first time an user is active\n",
    "         - lastevent_ts (timestamp) - last time an user is active\n",
    "\n",
    "         - init_days_interv (float) - days between registration and first activity\n",
    "         - tenure_days_interv (float) - days between registration and last activity\n",
    "         - active_days (float) - days the user has some activity on the platform\n",
    "         - session_h (float) - session's duration in hours\n",
    "\n",
    "     INPUT:\n",
    "         df (PySpark dataframe) - cleaned dataframe\n",
    "     OUTPUT:\n",
    "         df (PySpark dataframe) - dataframe with the listed features added\n",
    "    \"\"\"\n",
    "\n",
    "    # extract registration month and year from timestamp\n",
    "    df = df.withColumn(\"reg_date\", from_unixtime(col(\"reg_ts\"), \"MM-yyyy\"))\n",
    "\n",
    "    # create window: data grouped by userId, time ordered\n",
    "    win_user = (W.partitionBy(\"userId\")\n",
    "            .orderBy(\"log_ts\")\n",
    "            .rangeBetween(W.unboundedPreceding,\n",
    "                          W.unboundedFollowing))\n",
    "\n",
    "    # create window: data grouped by sessionId and userId, time ordered\n",
    "    win_user_session = (W.partitionBy(\"sessionId\", \"userId\")\n",
    "                        .orderBy(\"log_ts\")\n",
    "                        .rangeBetween(W.unboundedPreceding,\n",
    "                                      W.unboundedFollowing))\n",
    "\n",
    "    # record the first time an user is active\n",
    "    df = df.withColumn(\"firstevent_ts\", first(col(\"log_ts\")).over(win_user))\n",
    "    # record the last time an user is active\n",
    "    df = df.withColumn(\"lastevent_ts\", last(col(\"log_ts\")).over(win_user))\n",
    "\n",
    "    # warmup time = registration time to first event in days\n",
    "    df = df.withColumn(\"init_days_interv\",\n",
    "                       (col(\"firstevent_ts\").cast(\"long\")-col(\"reg_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # tenure time = registration time to last event in days\n",
    "    df = df.withColumn(\"tenure_days_interv\",\n",
    "                       (col(\"lastevent_ts\").cast(\"long\")-col(\"reg_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # active time =  days between the first event and the last event in days\n",
    "    df = df.withColumn(\"active_days\",\n",
    "                       (col(\"lastevent_ts\").cast(\"long\")-col(\"firstevent_ts\").cast(\"long\"))/(24*3600))\n",
    "\n",
    "    # create column that records the individual session's duration in hours\n",
    "    df = df.withColumn(\"session_h\",\n",
    "                    (last(df.log_ts).over(win_user_session) \\\n",
    "                     - first(df.log_ts).over(win_user_session))/3600)\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(\"reg_ts\", \"log_ts\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_features(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Features engineered to be used in modelling.\n",
    "\n",
    "        - nr_songs (int) - total number of songs user listened to\n",
    "        - nr_playlist (int) - number of songs added to the playlist\n",
    "\n",
    "        - nr_friends (int) - number of friends added through \"Add Friend\"\n",
    "\n",
    "        - nr_likes (int) - total number of \"Thumbs Up\" of the user\n",
    "        - nr_dislikes (int) - total number of \"Thumbs Down\" of the user\n",
    "\n",
    "        - nr_downgrades (int) - total number of visits to \"Downgrade\" page by the user\n",
    "        - nr_upgrades (int) - total number of visits to \"Upgrade\" page by the user\n",
    "\n",
    "        - nr_home (int) - total number of visits to \"Home\" page by the user\n",
    "        - nr_settings (int) - total number of visits to \"Settings\" page by the user\n",
    "\n",
    "        - nr_error (int) - total number of errors encountered by the user\n",
    "\n",
    "        - nr_ads (int) - total number of ads the user got\n",
    "        - nr_sessions (int) - number of sessions of the user\n",
    "        - n_acts (int) - total number of actions taken by the user\n",
    "\n",
    "        - avg_sess_h (float) - average session length in hours\n",
    "        - acts_per_session (float) - average number of actions per session for the user\n",
    "        - songs_per_session (float) - average numer of songs listened per session by the user\n",
    "        - ads_per_session (float) - average number of ads per session, received by user\n",
    "\n",
    "        - init_days_interv (int) - time interval in days from registration to the first action of the user\n",
    "        - tenure_days_interv (int) - time interval in days from registration to the last action of the user\n",
    "        - active_days (int) - number of days the user was active on the platform\n",
    "\n",
    "        - gender (binary) - 1 for F (female), 0 for M (male)\n",
    "        - level (binary) - 1 for paid, 0 for free\n",
    "\n",
    "        - churn (binary) - 1 for \"Cancellation Confirmation\" page visit, 0 otherwise\n",
    "\n",
    "    INPUT:\n",
    "        df (PySpark dataframe) - preprocessed dataframe\n",
    "    OUTPUT:\n",
    "        df_feats (PySpark dataframe) - dataframe that contains engineered features\n",
    "    \"\"\"\n",
    "\n",
    "    df_feats = df.groupBy(\"userId\") \\\n",
    "        .agg(\n",
    "\n",
    "            # count user's individual actions using all page visits\n",
    "\n",
    "            count(when(col(\"page\") == \"NextSong\", True)).alias(\"nr_songs\"),\n",
    "            count(when(col(\"page\") == \"Add to Playlist\", True)).alias(\"nr_playlist\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Add Friend\", True)).alias(\"nr_friends\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Thumbs Up\", True)).alias(\"nr_likes\"),                count(when(col(\"page\") == \"Thumbs Down\", True)).alias(\"nr_dislikes\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Downgrade\", True)).alias(\"nr_downgrades\"),\n",
    "            count(when(col(\"page\") == \"Upgrade\", True)).alias(\"nr_upgrades\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Home\", True)).alias(\"nr_home\"),\n",
    "            count(when(col(\"page\") == \"Settings\", True)).alias(\"nr_settings\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Error\", True)).alias(\"nr_error\"),\n",
    "\n",
    "            count(when(col(\"page\") == \"Roll Advert\", True)).alias(\"nr_ads\"),\n",
    "\n",
    "            # compute the number of sessions a user is in\n",
    "            countDistinct(\"sessionId\").alias(\"nr_sessions\"),\n",
    "\n",
    "            # find the total number of actions a user took\n",
    "            countDistinct(\"itemInSession\").alias(\"n_acts\"),\n",
    "\n",
    "            # compute the average session length in hours\n",
    "            avg(col(\"session_h\")).alias(\"avg_sess_h\"),\n",
    "\n",
    "            # compute the average number of page actions per sesssion - i.e. items in session\n",
    "            (countDistinct(\"itemInSession\") /countDistinct(\"sessionId\")).alias(\"acts_per_session\"),\n",
    "\n",
    "            # compute the average number of songs per session\n",
    "            (count(when(col(\"page\") == \"NextSong\", True)) /countDistinct(\"sessionId\")).alias(\"songs_per_session\"),\n",
    "\n",
    "            # compute the average number of ads per session\n",
    "             (count(when(col(\"page\") == \"Roll Advert\", True)) /countDistinct(\"sessionId\")).alias(\"ads_per_session\"),\n",
    "\n",
    "            # days between registration and first activity\n",
    "            first(col(\"init_days_interv\")).alias(\"init_days_interv\"),\n",
    "            # the tenure time on the platform: from registration to last event in days\n",
    "            first(col(\"tenure_days_interv\")).alias(\"tenure_days_interv\"),\n",
    "            # number of days user visited the platform, is active on the platform\n",
    "            first(col(\"active_days\")).alias(\"active_days\"),\n",
    "\n",
    "            # encode the gender 1 for F and 0 for M\n",
    "            first(when(col(\"gender\") == \"F\", 1).otherwise(0)).alias(\"gender\"),\n",
    "\n",
    "            # encode the level (paid/free) according to the last record\n",
    "            last(when(col(\"level\") == \"paid\", 1).otherwise(0)).alias(\"level\"),\n",
    "\n",
    "            # flag those users that downgraded\n",
    "            #last(when(col(\"page\") == \"Downgrade\", 1).otherwise(0)).alias(\"downgrade\"),\n",
    "\n",
    "            # create the churn column that records if the user cancelled\n",
    "            last(when(col(\"page\") == \"Cancellation Confirmation\", 1).otherwise(0)).alias(\"churn\"),\n",
    "            )\n",
    "\n",
    "    # columns to drop\n",
    "    drop_cols = (\"userId\", \"gender\", \"avg_sess_h\",\n",
    "                 \"nr_playlist\", \"nr_home\")\n",
    "    # drop the columns\n",
    "    #df_feats = df_feats.drop(\"userId\")\n",
    "    df_feats = df_feats.drop(*drop_cols)\n",
    "\n",
    "    # drop the null values\n",
    "    df_feats=df_feats.na.drop()\n",
    "\n",
    "    return df_feats\n",
    "\n",
    "\n",
    "def split_data (df):\n",
    "\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation set and test set.\n",
    "    Use a stratified sampling method.\n",
    "\n",
    "    INPUT:\n",
    "        df (PySpark dataframe) - dataframe\n",
    "    OUTPUT:\n",
    "        train_set, validation_set, test_set (PySpark dataframes) - \n",
    "                          percentage split based on the provided values\n",
    "    \"\"\"\n",
    "\n",
    "    # split dataframes between 0s and 1s\n",
    "    zeros = df.filter(df[\"churn\"]==0)\n",
    "    ones = df.filter(df[\"churn\"]==1)\n",
    "\n",
    "    # split dataframes into training and testing\n",
    "    train0, validation0, test0 = zeros.randomSplit(SPLIT_VALS, seed=1234)\n",
    "    train1, validation1, test1 = ones.randomSplit(SPLIT_VALS, seed=1234)\n",
    "\n",
    "    # stack datasets back together\n",
    "    train_set = train0.union(train1)\n",
    "    validation_set = validation0.union(validation1)\n",
    "    test_set = test0.union(test1)\n",
    "\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "def prepare_data(dataset_filepath):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that combines all the data preparation steps.\n",
    "    \n",
    "    INPUT:\n",
    "        dataset_filepath (str) - filepath for raw data json file\n",
    "    OUTPUT:\n",
    "       train_set, validation_set, test_set (PySpark dataframes) - \n",
    "            subsets of the features dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Load data...')\n",
    "    df = load_data(dataset_filepath)\n",
    "    \n",
    "    print('Clean data...')\n",
    "    df_clean = clean_data(df)\n",
    "    \n",
    "    print('Preprocess data...')\n",
    "    df_proc = preprocess_data(df_clean)\n",
    "    \n",
    "    print('Create features dataset...')\n",
    "    df_feats = build_features(df_proc)\n",
    "    \n",
    "    print('Split the features dataset...')\n",
    "    train_set, validation_set, test_set = split_data(df_feats)\n",
    "    \n",
    "    return train_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2c99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the features and the label\n",
    "CAT_FEATURES = [\"level\"]\n",
    "CONT_FEATURES = [\"nr_songs\", \"nr_likes\", \"nr_dislikes\", \"nr_friends\", \"nr_downgrades\",\n",
    "                \"nr_upgrades\", \"nr_error\", \"nr_settings\", \"nr_ads\", \"nr_sessions\",\n",
    "                \"n_acts\", \"acts_per_session\", \"songs_per_session\", \"ads_per_session\",\n",
    "                \"init_days_interv\", \"tenure_days_interv\", \"active_days\"]\n",
    "CHURN_LABEL = \"churn\"\n",
    "\n",
    "def build_data_pipeline():\n",
    "    \"\"\"\n",
    "    Combines all the stages of the data processing.\n",
    "    \"\"\"\n",
    "    # stages in the pipeline\n",
    "    stages = [] \n",
    "    \n",
    "    # encode the labels\n",
    "    label_indexer =  StringIndexer(inputCol=CHURN_LABEL, outputCol=\"label\")\n",
    "    stages += [label_indexer]\n",
    "    \n",
    "    # encode the binary features\n",
    "    bin_assembler = VectorAssembler(inputCols=CAT_FEATURES, outputCol=\"bin_features\")\n",
    "    stages += [bin_assembler]\n",
    "    \n",
    "    # encode the continuous features\n",
    "    cont_assembler = VectorAssembler(inputCols = CONT_FEATURES, outputCol=\"cont_features\")\n",
    "    stages += [cont_assembler]\n",
    "    # normalize the continuous features\n",
    "    cont_scaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaler\", \n",
    "                                 withStd=True , withMean=True)\n",
    "    stages += [cont_scaler]\n",
    "    \n",
    "    # pass all to the vector assembler to create a single sparse vector\n",
    "    all_assembler = VectorAssembler(inputCols=[\"bin_features\", \"cont_scaler\"],  \n",
    "                            outputCol=\"features\")\n",
    "    stages += [all_assembler]\n",
    "    \n",
    "    # add the models to the pipeline\n",
    "    #stages += [models]\n",
    "    \n",
    "    # create a pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cbf18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base models\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', \n",
    "                        predictionCol='pred_lr', probabilityCol='prob_lr',\n",
    "                        rawPredictionCol='rawPred_lr')\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', \n",
    "                  predictionCol='pred_rf', probabilityCol='prob_rf', \n",
    "                  rawPredictionCol='rawPred_rf')\n",
    "\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='label', \n",
    "                            predictionCol='pred_gbt')\n",
    "\n",
    "layers=[18,8,4,2]\n",
    "mlpc= MultilayerPerceptronClassifier(featuresCol='features', labelCol='label',\n",
    "                                     predictionCol='pred_mlpc', probabilityCol='prob_mlpc',\n",
    "                                     rawPredictionCol='rawPred_mlpc', layers=layers)\n",
    "models = [lr, rf, gbt, mlpc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398c54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_VALS = [.4, .3, .3]   \n",
    "\n",
    "# path for the test set file file\n",
    "#path_dataset = \"data/mini_sparkify_event_data.json\"\n",
    "path_dataset = \"data/full_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa710a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Loading the dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is loaded...\n",
      "Clean data...\n",
      "Cleaning the data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 26259199 rows initially.\n",
      "Finished cleaning the data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset has 26259199 rows, 0 rows were removed\n",
      "Preprocess data...\n",
      "Create features dataset...\n",
      "Split the features dataset...\n"
     ]
    }
   ],
   "source": [
    "# clean, process and split the data\n",
    "train_df, validation_df, test_df = prepare_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affb1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data in the memory and disk\n",
    "train_cached = train_df.persist()\n",
    "validation_cached = validation_df.persist()\n",
    "test_cached = test_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae4a8c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# instantiate the data pipeline\n",
    "data_pipeline = build_data_pipeline()\n",
    "\n",
    "# prepare the datasets for modeling\n",
    "\n",
    "data_train_pipeline_model = data_pipeline.fit(train_cached)\n",
    "train_transf = data_train_pipeline_model.transform(train_cached)\n",
    "\n",
    "data_validation_pipeline_model = data_pipeline.fit(validation_cached)\n",
    "validation_transf = data_validation_pipeline_model.transform(validation_cached)\n",
    "\n",
    "data_test_pipeline_model = data_pipeline.fit(test_cached)\n",
    "test_transf = data_test_pipeline_model.transform(test_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dadb0475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[nr_songs: bigint, nr_friends: bigint, nr_likes: bigint, nr_dislikes: bigint, nr_downgrades: bigint, nr_upgrades: bigint, nr_settings: bigint, nr_error: bigint, nr_ads: bigint, nr_sessions: bigint, n_acts: bigint, acts_per_session: double, songs_per_session: double, ads_per_session: double, init_days_interv: double, tenure_days_interv: double, active_days: double, level: int, churn: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the datasets from memory\n",
    "train_cached.unpersist()\n",
    "test_cached.unpersist()\n",
    "validation_cached.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3462bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the transformed set in the memory\n",
    "train_transf_cached = train_transf.persist()\n",
    "validation_transf_cached = validation_transf.persist()\n",
    "test_transf_cached = test_transf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20e130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time......... 2.809 min\n"
     ]
    }
   ],
   "source": [
    "# build pipeline to generate predictions from base classifiers\n",
    "base_pipeline = Pipeline(stages=models)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# fit the pipeline on the transformed set\n",
    "base_pipeline_model = base_pipeline.fit(train_transf_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time \n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4823b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the validation set\n",
    "base_pred = base_pipeline_model.transform(validation_transf_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba193a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[nr_songs: bigint, nr_friends: bigint, nr_likes: bigint, nr_dislikes: bigint, nr_downgrades: bigint, nr_upgrades: bigint, nr_settings: bigint, nr_error: bigint, nr_ads: bigint, nr_sessions: bigint, n_acts: bigint, acts_per_session: double, songs_per_session: double, ads_per_session: double, init_days_interv: double, tenure_days_interv: double, active_days: double, level: int, churn: int, label: double, bin_features: vector, cont_features: vector, cont_scaler: vector, features: vector]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear data from memory\n",
    "train_transf_cached.unpersist()\n",
    "validation_transf_cached.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e809ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the meta features dataset\n",
    "meta_features_df = base_pred.select(\"pred_lr\", \"pred_rf\", \"pred_gbt\", \"pred_mlpc\",\"label\",\n",
    "                                   \"prob_lr\", \"prob_rf\", \"prob_mlpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b068d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the meta_features for modeling\n",
    "# split the features and the label\n",
    "META_FEATURES = [\"pred_lr\", \"pred_rf\",\"pred_gbt\", \"pred_mlpc\"]\n",
    "META_CONT_FEATURES = [\"prob_lr\", \"prob_rf\", \"prob_mlpc\"]\n",
    "META_LABEL = \"label\"\n",
    "\n",
    "def build_meta_pipeline():\n",
    "    \"\"\"\n",
    "    Combines all the stages of the meta features processing.\n",
    "    \"\"\"\n",
    "    # stages in the pipeline\n",
    "    stages = [] \n",
    "    \n",
    "    # encode the labels\n",
    "    label_indexer =  StringIndexer(inputCol=META_LABEL, outputCol=\"meta_label\")\n",
    "    stages += [label_indexer]\n",
    "    \n",
    "    # encode the binary features\n",
    "    bin_assembler = VectorAssembler(inputCols=META_FEATURES, outputCol=\"bin_features\")\n",
    "    stages += [bin_assembler]\n",
    "    \n",
    "    # encode the continuous features\n",
    "    cont_assembler = VectorAssembler(inputCols = META_CONT_FEATURES, outputCol=\"cont_features\")\n",
    "    stages += [cont_assembler]\n",
    "    # normalize the continuous features\n",
    "    cont_scaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaler\", \n",
    "                                 withStd=True , withMean=True)\n",
    "    stages += [cont_scaler]\n",
    "    \n",
    "    # pass all to the vector assembler to create a single sparse vector\n",
    "    all_assembler = VectorAssembler(inputCols=[\"bin_features\"],  \n",
    "                                    outputCol=\"meta_features\")\n",
    "    stages += [all_assembler]\n",
    "    \n",
    "    # add the models to the pipeline\n",
    "    #stages += [models]\n",
    "    \n",
    "    # create a pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c2f7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# instantiate the data pipeline\n",
    "meta_pipeline = build_meta_pipeline()\n",
    "\n",
    "# prepare the datasets for modeling\n",
    "\n",
    "meta_pipeline_model = meta_pipeline.fit(meta_features_df)\n",
    "meta_transf = meta_pipeline_model.transform(meta_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6020552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train the meta clasifier\n",
    "meta_lr = LogisticRegression(featuresCol='meta_features', \n",
    "                             labelCol='label', \n",
    "                             predictionCol='final_pred')\n",
    "meta_classifier = meta_lr.fit(meta_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d50a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "meta_dt = LogisticRegression(featuresCol='meta_features', \n",
    "                             labelCol='label', \n",
    "                             predictionCol='final_pred',\n",
    "                                maxIter=400, elasticNetParam=0.1, regParam=0)\n",
    "meta_classifier_dt = meta_dt.fit(meta_transf.persist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50d1c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "meta_dt = RandomForestClassifier(featuresCol='meta_features', \n",
    "                             labelCol='label', \n",
    "                             predictionCol='final_pred')\n",
    "meta_classifier_dt = meta_dt.fit(meta_transf.persist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "064a2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classifiers make predictions on the test set\n",
    "test_pred = base_pipeline_model.transform(test_transf_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c1eeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the meta features test dataset\n",
    "meta_features_test_df = test_pred.select(\"pred_lr\", \"pred_rf\", \"pred_gbt\", \"pred_mlpc\",\"label\",\n",
    "                                        \"prob_lr\", \"prob_rf\", \"prob_mlpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "433afab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# prepare the datasets for modeling\n",
    "meta_pipeline_test = meta_pipeline.fit(meta_features_test_df)\n",
    "meta_test_transf = meta_pipeline_test.transform(meta_features_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af7834bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "test_pred = meta_classifier.transform(meta_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1ce709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "test_pred_dt = meta_classifier_dt.transform(meta_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "278bdff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pred_lr: double (nullable = false)\n",
      " |-- pred_rf: double (nullable = false)\n",
      " |-- pred_gbt: double (nullable = false)\n",
      " |-- pred_mlpc: double (nullable = false)\n",
      " |-- label: double (nullable = false)\n",
      " |-- prob_lr: vector (nullable = true)\n",
      " |-- prob_rf: vector (nullable = true)\n",
      " |-- prob_mlpc: vector (nullable = true)\n",
      " |-- meta_label: double (nullable = false)\n",
      " |-- bin_features: vector (nullable = true)\n",
      " |-- cont_features: vector (nullable = true)\n",
      " |-- cont_scaler: vector (nullable = true)\n",
      " |-- meta_features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- final_pred: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred_dt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29c9a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_meta_classifier = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", \n",
    "                                          metricName= \"areaUnderPR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "387f65fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127750816797253\n"
     ]
    }
   ],
   "source": [
    "print(evaluator_meta_classifier.evaluate(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74d2caeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2375:============================================>      (351 + 12) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127750816797253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(evaluator_meta_classifier.evaluate(test_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1e7c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator1_meta_classifier = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", \n",
    "                                          metricName= \"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bd9bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.891974796037296\n"
     ]
    }
   ],
   "source": [
    "print(evaluator1_meta_classifier.evaluate(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f56dff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2404:===================================================>(398 + 2) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.891974796037296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(evaluator1_meta_classifier.evaluate(test_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8895e",
   "metadata": {},
   "source": [
    "## <font color='blue'>Modeling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ab7eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60950aa7",
   "metadata": {},
   "source": [
    "### <font color='blue'>Baseline Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd200879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of churn users in each set\n",
    "fn_train = train_cached.where(train_cached.churn==1).count()\n",
    "fn_test = test_cached.where(test_cached.churn==1).count()\n",
    "\n",
    "# count the number of not churn users in each set\n",
    "tn_train = train_cached.where(train_cached.churn==0).count()\n",
    "tn_test = test_cached.where(test_cached.churn==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of ZeroR model on train set\n",
    "accuracy_train = tn_train/(fn_train+tn_train)\n",
    "print(\"With fp = tp = 0, fn = {} and tn = {}, the accuracy of the ZeroR model on the train set is {}%.\"\n",
    "      .format(fn_train, tn_train, round(accuracy_train,4)*100))\n",
    "\n",
    "# accuracy of ZeroR model on test set\n",
    "accuracy_test = tn_test/(fn_test+tn_test)\n",
    "print(\"With fp = tp = 0, fn = {} and tn = {}, the accuracy of the ZeroR model on the test set is {}%.\"\n",
    "      .format(fn_test, tn_test, round(accuracy_test,4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d578b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Build a ZeroR baseline model. This is a simple model that always predicts the most numerous class. The accuracy of this model on the test set is:\n",
    "\n",
    "$${\\rm accuracy\\;testset} = \\frac{\\rm not\\; churn}{\\rm all\\; users} = \\frac{46}{58} = 0.79$$\n",
    "    \n",
    "Any classifier we build must have better accuracy on the test set than the accuracy of this dummy classifier.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd4b3e",
   "metadata": {},
   "source": [
    "### <font color='blue'>Build Models Evaluators</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c3a5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCol=\"label\"\n",
    "predCol=\"predictions\"\n",
    "featuresCol=\"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b55ea7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute relevant metrics for binary classification\n",
    "def conf_metrics(dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "        Calculates the metrics associated to the confusion matrix.\n",
    "\n",
    "        INPUT:\n",
    "            dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "        OUTPUT:\n",
    "            accuracy (float) - metric\n",
    "            precision (float) - metric\n",
    "            recall (float) - metric\n",
    "            F1 (float) - metric\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # calculate the elements of the confusion matrix\n",
    "    tn = dataset.where((dataset[labelCol]==0) & (dataset[predCol]==0)).count()\n",
    "    tp = dataset.where((dataset[labelCol]==1) & (dataset[predCol]==1)).count()                   \n",
    "    fn = dataset.where((dataset[labelCol]==1) & (dataset[predCol]==0)).count()                   \n",
    "    fp = dataset.where((dataset[labelCol]==0) & (dataset[predCol]==1)).count()\n",
    "    \n",
    "    # calculate accuracy, precision, recall, and F1-score\n",
    "    accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 =  2 * (precision*recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cc242b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display the metrics of interest\n",
    "def display_metrics(dataset, evaluator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints evaluation metrics for the model. \n",
    "    \n",
    "    INPUT:\n",
    "         dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = conf_metrics(dataset)[0]\n",
    "    precision = conf_metrics(dataset)[1]\n",
    "    recall = conf_metrics(dataset)[2]\n",
    "    f1 = conf_metrics(dataset)[3]\n",
    "\n",
    "    # calculate auc metrics\n",
    "    roc_cl = evaluator.evaluate(dataset, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    pr_cl = evaluator.evaluate(dataset, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    dataset.groupBy(dataset[labelCol], dataset[predCol]).count().show()\n",
    "    print(\"\")\n",
    "    print(\"accuracy...............%6.3f\" % accuracy)\n",
    "    print(\"precision..............%6.3f\" % precision)\n",
    "    print(\"recall.................%6.3f\" % recall)\n",
    "    print(\"F1.....................%6.3f\" % f1)\n",
    "    print(\"auc_roc................%6.3f\" % roc_cl)\n",
    "    print(\"auc_pr.................%6.3f\" % pr_cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "701b8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python\n",
    "\n",
    "def plot_roc_pr_curves(predictions, model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates ROC-AUC and PR-AUC scores and plots the ROC and PR curves.\n",
    "    \n",
    "    INPUT:\n",
    "        predictions (PySpark dataframe) - contains probability predictions, label column\n",
    "        model_name (str) - classifier name\n",
    "        \n",
    "    OUTPUT:\n",
    "        none - two plots are displayed\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # transform predictions PySpark dataframe into Pandas dataframe\n",
    "    pred_pandas = predictions.select(predictions.label, predictions.probability).toPandas()\n",
    "    \n",
    "    # calculate roc_auc score\n",
    "    roc_auc = roc_auc_score(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(pred_pandas.label))]\n",
    "    # calculate roc curves\n",
    "    fpr, tpr, _ = roc_curve(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(pred_pandas.label, ns_probs)\n",
    "    \n",
    "    # calculate precision, recall for each threshold\n",
    "    precision, recall, _ = precision_recall_curve(pred_pandas.label, pred_pandas.probability.str[1])\n",
    "    # calculate pr auc score\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "\n",
    "    # create figure which contains two subplots\n",
    "    plt.figure(figsize=[12,6])\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    \n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    plt.plot(fpr, tpr, marker='.', color='firebrick', label='ROC AUC = %.3f' % (roc_auc))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"ROC Curve:\" + model_name)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    # plot the precision-recall curves\n",
    "    \n",
    "    ns_line = len(pred_pandas[pred_pandas.label==1]) / len(pred_pandas.label)\n",
    "    plt.plot([0, 1], [ns_line, ns_line], linestyle='--', label='No Skill')\n",
    "    plt.plot(recall, precision, marker='.', color='firebrick', label='PR AUC = %.3f' % (pr_auc))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"Precision-Recall Curve:\" + model_name)\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c147aee",
   "metadata": {},
   "source": [
    "### <font color='blue'>Build Pipelines</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6de972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement K-fold cross validation and grid search \n",
    "\n",
    "def grid_search_model(pipeline, param):\n",
    "    \"\"\"\n",
    "    Creates a cross validation object and performs grid search\n",
    "    over a set of parameters.\n",
    "    \n",
    "    INPUT:\n",
    "        param = grid of parameters\n",
    "        pipeline = model pipeline \n",
    "    \n",
    "    OUTPUT:\n",
    "        cv = cross validation object\n",
    "    \"\"\"\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=param,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5,\n",
    "                    parallelism=2)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50124f",
   "metadata": {},
   "source": [
    "## <font color='blue'>Evaluate PySpark Classifiers</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a550beb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Evaluate binary classifiers implemented in PySpark on the train set, using default parameters to select the best performing classifiers to be tuned in the next stage.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8461eef",
   "metadata": {},
   "source": [
    "### <font color='blue'>Logistic Regression Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b591c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training LOGISTIC REGRESSION CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "labelCol=\"label\"\n",
    "predCol=\"prediction\"\n",
    "\n",
    "# instantiate the classifier\n",
    "lr_classifier = LogisticRegression(labelCol = \"label\",\n",
    "                                       featuresCol = \"features\")\n",
    "\n",
    "# build specific pipeline\n",
    "lr_pipeline = build_full_pipeline(lr_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"label\")\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_lr = lr_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_lr = model_lr.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "#roc_lr = evaluator.evaluate(predictions_lr, {evaluator.metricName: \"areaUnderROC\"})\n",
    "#pr_lr = evaluator.evaluate(predictions_lr, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_lr, prec_lr, rec_lr, f1_lr = conf_metrics(predictions_lr)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Metrics on the train set\")\n",
    "display_metrics(predictions_lr, evaluator)\n",
    "print(\"\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_lr, \"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f85bac",
   "metadata": {},
   "source": [
    "### <font color='blue'>Decision Trees Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training DECISION TREES CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "# build specific pipeline\n",
    "dt_classifier = DecisionTreeClassifier(labelCol = \"label\",\n",
    "                                           featuresCol = \"features\",\n",
    "                                           seed=1234)\n",
    "dt_pipeline = build_full_pipeline(dt_classifier)\n",
    "\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"label\")\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_dt = dt_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_dt = model_dt.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_dt = evaluator.evaluate(predictions_dt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_dt = evaluator.evaluate(predictions_dt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_dt, prec_dt, rec_dt, f1_dt = conf_metrics(predictions_dt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Metrics on the train set\")\n",
    "display_metrics(predictions_dt, roc_dt, pr_dt)\n",
    "print(\"\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_dt, \"DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c3700",
   "metadata": {},
   "source": [
    "### <font color='blue'>Random Forest Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f94c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RANDOM FOREST CLASSIFIER\n",
      "\n",
      "\n",
      "Training time......... 0.044 min\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labelCol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9976/3878782303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# record the confusion matrix metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0macc_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# print all evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9976/2300847277.py\u001b[0m in \u001b[0;36mconf_metrics\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# calculate the elements of the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labelCol' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(f\"Training RANDOM FOREST CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "rf_classifier = RandomForestClassifier(labelCol = \"label\",\n",
    "                                           featuresCol = \"features\", \n",
    "                                           seed=1234)\n",
    "# build the specific pipeline\n",
    "rf_pipeline = build_data_pipeline(rf_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"label\")\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_rf = rf_pipeline.fit(df_train)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_rf = model_rf.transform(df_train)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_rf, prec_rf, rec_rf, f1_rf = conf_metrics(predictions_rf)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Metrics on the train set\")\n",
    "display_metrics(predictions_rf, roc_rf, pr_rf)\n",
    "print(\"\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_rf, \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09ea7c",
   "metadata": {},
   "source": [
    "### <font color='blue'>Gradient Boosted Trees</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aea9598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GRADIENT BOOSTED TREES\n",
      "\n",
      "\n",
      "Training time......... 0.073 min\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"predictions\" among (active_days, acts_per_session, ads_per_session, churn, init_days_interv, level, n_acts, nr_ads, nr_dislikes, nr_downgrades, nr_error, nr_friends, nr_likes, nr_sessions, nr_settings, nr_songs, nr_upgrades, songs_per_session, tenure_days_interv, label, bin_features, cont_features, cont_scaler, features, rawPrediction, probability, prediction)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9976/1618027505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# record the confusion matrix metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0macc_gbt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_gbt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_gbt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_gbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_gbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# print all evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9976/2300847277.py\u001b[0m in \u001b[0;36mconf_metrics\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# calculate the elements of the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysparkenv/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \"\"\"\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysparkenv/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysparkenv/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"predictions\" among (active_days, acts_per_session, ads_per_session, churn, init_days_interv, level, n_acts, nr_ads, nr_dislikes, nr_downgrades, nr_error, nr_friends, nr_likes, nr_sessions, nr_settings, nr_songs, nr_upgrades, songs_per_session, tenure_days_interv, label, bin_features, cont_features, cont_scaler, features, rawPrediction, probability, prediction)"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(f\"Training GRADIENT BOOSTED TREES\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "gbt_classifier = GBTClassifier(labelCol = \"label\",\n",
    "                                featuresCol = \"features\",\n",
    "                                seed=1234)\n",
    "# build specific pipeline\n",
    "gbt_pipeline = build_data_pipeline(gbt_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"label\")\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_gbt = gbt_pipeline.fit(df_train)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_gbt = model_gbt.transform(df_train)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_gbt, prec_gbt, rec_gbt, f1_gbt = conf_metrics(predictions_gbt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Metrics on the train set\")\n",
    "display_metrics(predictions_gbt, roc_gbt, pr_gbt)\n",
    "print(\"\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_gbt, \"GBT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc397ba",
   "metadata": {},
   "source": [
    "### <font color='blue'>Multilayer Perceptron Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c932fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training MULTILAYER PERCEPTRON CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "# specify layers: 19 (features), two intermediate (8, 4), output 2 (classes)\n",
    "layers=[18, 8, 4, 2]\n",
    "# create the trainer and set its parameters\n",
    "mlpc_classifier = MultilayerPerceptronClassifier(labelCol = \"label\",\n",
    "                                                featuresCol = \"features\",\n",
    "                                                layers=layers,\n",
    "                                                seed=1234)\n",
    "# build specific pipeline\n",
    "mlpc_pipeline = build_full_pipeline(mlpc_classifier)\n",
    "\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"label\")\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_mlpc = mlpc_pipeline.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_mlpc = model_mlpc.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc = conf_metrics(predictions_mlpc)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Metrics on the train set\")\n",
    "display_metrics(predictions_mlpc, roc_mlpc, pr_mlpc)\n",
    "print(\"\")\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_mlpc, \"MLPC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540f4dc",
   "metadata": {},
   "source": [
    "### <font color='blue'>Linear Support Vector Machine</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training LINEAR SUPPORT VECTOR MACHINE\")\n",
    "print(\"\")\n",
    "\n",
    "# instantiate the classifier\n",
    "lsvc_classifier = LinearSVC(labelCol = \"label\",\n",
    "                            featuresCol = \"features\")\n",
    "# build specific pipeline\n",
    "lsvc_pipeline = build_full_pipeline(lsvc_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"label\")\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_lsvc = lsvc.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_lsvc = model_lsvc.transform(train_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_lsvc = evaluator.evaluate(predictions_lsvc, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_lsvc = evaluator.evaluate(predictions_lsvc, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_lsvc, prec_lsvc, rec_lsvc, f1_lsvc = conf_metrics(predictions_lsvc)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Metrics on the train set\")\n",
    "display_metrics(predictions_lsvc, roc_lsvc, pr_lsvc)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d5d37",
   "metadata": {},
   "source": [
    "### <font color='blue'>Choose the best classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebf49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Pandas dataframe with metrics\n",
    "dict_metrics = {\"LinReg\": [acc_lr, prec_lr, rec_lr, f1_lr, roc_lr, pr_lr],\n",
    "                \"DecTrees\": [acc_dt, prec_dt, rec_dt, f1_dt, roc_dt, pr_dt], \n",
    "                \"RandForest\": [acc_rf, prec_rf, rec_rf, f1_rf, roc_rf, pr_rf],\n",
    "                \"GradBoost\": [acc_gbt, prec_gbt, rec_gbt, f1_gbt, roc_gbt, pr_gbt],\n",
    "                \"MultiLPerceptron\": [acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc, roc_mlpc, pr_mlpc],\n",
    "                \"LinearSVM\": [acc_lsvc, prec_lsvc, rec_lsvc, f1_lsvc, roc_lsvc, pr_lsvc],\n",
    "                \"list_metrics\" : [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc_roc\", \"auc_pr\"]\n",
    "               }\n",
    "df_mets = pd.DataFrame.from_dict(dict_metrics).set_index(\"list_metrics\")\n",
    "df_mets.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f805cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust figure size and font size\n",
    "sns.set(rc = {\"figure.figsize\":(16,2)})\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "ax = df_mets.plot.bar(y=[\"LinReg\", \"DecTrees\", \"RandForest\", \"GradBoost\", \"MultiLPerceptron\", \"LinearSVM\"], rot=0)\n",
    "\n",
    "# create title and labels\n",
    "plt.title(\"Confusion Matrix Metrics For Sparkify Small Train Dataset\")\n",
    "ax.set_xlabel(\"\");\n",
    "ax.set_ylabel(\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea1bc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "It is clear from the table of performance metrics that Random Forest, Gradient Boosted Trees and MultiLayer Perceptron perform the best. We will use grid search with cross validation to tune Random Forest and Gradient Boosted Trees. This dataset is too small for a neural network to perform optimally, so we will skip the MultiLayer Perceptron in this case.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799e4ff",
   "metadata": {},
   "source": [
    "## <font color='blue'>Tune Hyperparameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4710595",
   "metadata": {},
   "source": [
    "### <font color='blue'>Random Forest Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b650326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Random Forest\"\n",
    "def rf_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Random Forest\n",
    "    param_rf = ParamGridBuilder()\n",
    "    param_rf = param_rf.addGrid(model.maxDepth, [5, 10, 15, 20, 25]) \n",
    "    param_rf = param_rf.addGrid(model.maxBins, [8, 16, 24, 32, 48])\n",
    "    param_rf = param_rf.addGrid(model.numTrees, [10, 20, 40, 60, 80]) \n",
    "    param_rf = param_rf.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_rf)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training RANDOM FOREST CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# instantiate the classifier\n",
    "rf_classifier = RandomForestClassifier(labelCol = \"label\",\n",
    "                                           featuresCol = \"features\", \n",
    "                                           seed=1234)\n",
    "# build the specific pipeline\n",
    "rf_pipeline = build_full_pipeline(rf_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "rf = rf_grid_search(rf_pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_rf = rf.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_rf = model_rf.bestModel.transform(test_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_rf, prec_rf, rec_rf, f1_rf = conf_metrics(predictions_rf)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Evaluation metrics on the test set\")\n",
    "display_metrics(predictions_rf, roc_rf, pr_rf)\n",
    "print(\"\")\n",
    "\n",
    "best_param_rf = list(model_rf.getEstimatorParamMaps()[np.argmax(model_rf.avgMetrics)].values())\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"maxDepth:..........\", best_param_rf[0])\n",
    "print(\"maxBins:...........\", best_param_rf[1])\n",
    "print(\"numTrees:..........\", best_param_rf[2])\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_rf, \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <font color='blue'>Gradient Boosted Trees</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f35cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Gradient Boosted Trees\"\n",
    "def gbt_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Gradient Boosted Trees\n",
    "    param_gbt = ParamGridBuilder()\n",
    "    param_gbt = param_gbt.addGrid(model.maxDepth, [5, 10, 15, 20])\n",
    "    param_gbt = param_gbt.addGrid(model.maxIter, [20, 40, 60, 80, 100])\n",
    "    param_gbt = param_gbt.addGrid(model.stepSize, [.05, .1, .15, .2])\n",
    "    param_gbt = param_gbt.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_gbt)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf83d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training GRADIENT BOOSTED TREES\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# instantiate the classifier\n",
    "gbt_classifier = GBTClassifier(labelCol = \"label\",\n",
    "                                featuresCol = \"features\",\n",
    "                                seed=1234)\n",
    "# build specific pipeline\n",
    "gbt_pipeline = build_full_pipeline(gbt_classifier)\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "gbt = gbt_grid_search(gbt_pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_gbt = gbt.fit(train_cached)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_gbt = model_gbt.bestModel.transform(test_cached)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_gbt, prec_gbt, rec_gbt, f1_gbt = conf_metrics(predictions_gbt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "print(\"Evaluation metrics on the test set\")\n",
    "display_metrics(predictions_gbt, roc_gbt, pr_gbt)\n",
    "print(\"\")\n",
    "\n",
    "# print the best parameters from the grid\n",
    "best_model_gbt = model_gbt.bestModel.stages[-1]\n",
    "param_gbt1 = best_model_gbt.getMaxDepth()\n",
    "param_gbt2 = best_model_gbt.getMaxIter()\n",
    "param_gbt3 = best_model_gbt.getStepSize()\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"maxDepth:..........\", param_gbt1)\n",
    "print(\"maxIter:...........\", param_gbt2)\n",
    "print(\"stepSize:..........\", param_gbt3)\n",
    "\n",
    "# plot the ROC and PR curves\n",
    "plot_roc_pr_curves(predictions_gbt, \"GBT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061954d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_pr_two_curves(predictions_model1, predictions_model2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plots the ROC and PR curves for two models on the same graphs.\n",
    "    \n",
    "    INPUT:\n",
    "        predictions_model1 (PySpark dataframe) - contains probability predictions for the first model\n",
    "        predictions_model2 (PySpark dataframe) - contains probability predictions for the second model\n",
    "        \n",
    "    OUTPUT:\n",
    "        none - two plots are displayed\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # transform predictions PySpark dataframe into Pandas dataframe\n",
    "    pred1_pandas = predictions_model1.select(predictions_model1.label, \n",
    "                                             predictions_model1.probability).toPandas()\n",
    "    pred2_pandas = predictions_model2.select(predictions_model2.label, \n",
    "                                             predictions_model2.probability).toPandas()\n",
    "    \n",
    "    # calculate roc_auc scores for first model\n",
    "    roc_auc1 = roc_auc_score(pred1_pandas.label, pred1_pandas.probability.str[1])\n",
    "    # calculate roc_auc scores for second model\n",
    "    roc_auc2 = roc_auc_score(pred2_pandas.label, pred2_pandas.probability.str[1])\n",
    "    \n",
    "    # calculate roc curves for model 1\n",
    "    fpr1, tpr1, _ = roc_curve(pred1_pandas.label, pred1_pandas.probability.str[1])\n",
    "    # calculate roc curves for model 2\n",
    "    fpr2, tpr2, _ = roc_curve(pred2_pandas.label, pred2_pandas.probability.str[1])\n",
    "    \n",
    "    # calculate precision, recall for each threshold for the first model\n",
    "    precision1, recall1, _ = precision_recall_curve(pred1_pandas.label, pred1_pandas.probability.str[1])\n",
    "    # calculate pr auc score\n",
    "    pr_auc1 = auc(recall1, precision1)\n",
    "    \n",
    "    # calculate precision, recall for each threshold for the second model\n",
    "    precision2, recall2, _ = precision_recall_curve(pred2_pandas.label, pred2_pandas.probability.str[1])\n",
    "    # calculate pr auc score\n",
    "    pr_auc2 = auc(recall2, precision2)\n",
    "\n",
    "    # create figure which contains two subplots\n",
    "    plt.figure(figsize=[12,6])\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    \n",
    "    # plot the roc curve for the model1\n",
    "    plt.plot(fpr1, tpr1, marker='.', color='firebrick', label='RF: ROC-AUC = %.3f' % (roc_auc1))\n",
    "    plt.plot(fpr2, tpr2, marker='.', color='green', label='GBT: ROC-AUC = %.3f' % (roc_auc2))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"ROC Curves\")\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    \n",
    "    # plot the precision-recall curves\n",
    "    plt.plot(recall1, precision1, marker='.', color=\"firebrick\", label='RF: PR-AUC = %.3f' % (pr_auc1))\n",
    "    plt.plot(recall2, precision2, marker='.', color=\"green\", label='GBT: PR-AUC = %.3f' % (pr_auc2))\n",
    "    \n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # figure title\n",
    "    plt.title(\"Precision-Recall Curves\")\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_two_curves(predictions_rf, predictions_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Pandas dataframe with metrics\n",
    "dict_metrics = {\"RandForest\": [acc_rf, prec_rf, rec_rf, f1_rf, roc_rf, pr_rf],\n",
    "                \"GradBoost\": [acc_gbt, prec_gbt, rec_gbt, f1_gbt, roc_gbt, pr_gbt],\n",
    "                \"list_metrics\" : [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc_roc\", \"auc_pr\"]\n",
    "               }\n",
    "df_mets = pd.DataFrame.from_dict(dict_metrics).set_index(\"list_metrics\")\n",
    "df_mets.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust figure size and font size\n",
    "sns.set(rc = {\"figure.figsize\":(16,2)})\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "ax = df_mets.plot.bar(y=[\"RandForest\", \"GradBoost\"], rot=0)\n",
    "\n",
    "# create title and labels\n",
    "plt.title(\"Confusion Matrix Metrics For Sparkify Small Train Dataset\")\n",
    "ax.set_xlabel(\"\");\n",
    "ax.set_ylabel(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf9508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
