{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcabe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f675e4e556bd4fd3a2ce35efad4636d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1640238224789_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-5-244.us-east-2.compute.internal:20888/proxy/application_1640238224789_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-4-34.us-east-2.compute.internal:8042/node/containerlogs/container_1640238224789_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available AWS EMER Memory: 11171M"
     ]
    }
   ],
   "source": [
    "# check the available memory\n",
    "print('Available AWS EMER Memory: {}'.format(spark.sparkContext.getConf().get('spark.driver.memory')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c6003b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4dc1252c7743f0b6e382b6eef9b0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/ab/ea76361f9d3e732e114adcd801d2820d5319c23d0ac5482fa3b412db217e/pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==0.25.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl (247kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.2\n",
      "\n",
      "Collecting matplotlib==3.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/19/7a/60bd79c5d79559150f8bba866dd7d434f0a170312e4d15e8aefa5faba294/matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1640238517890-0/lib/python3.7/site-packages (from matplotlib==3.1.1)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib==3.1.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/34/895006117f6fce0b4de045c87e154ee4a20c68ec0a4c9a36d900888fb6bc/pyparsing-3.0.6-py3-none-any.whl (97kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.1.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/f9/695d6bedebd747e5eb0fe8fad57b72fdf25411273a39791cde838d5a8f51/cycler-0.11.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.7/site-packages (from matplotlib==3.1.1)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib==3.1.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/6b/6e567cb2e86d4e5939a9233f8734e26021b6a9c1bc4b1edccba236a84cc2/kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.1)\n",
      "Installing collected packages: pyparsing, cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.3.2 matplotlib-3.1.1 pyparsing-3.0.6"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\")\n",
    "sc.install_pypi_package(\"matplotlib==3.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9734fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acb0aa58a3642fc9b4208821fdd4f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import neccessary libraries and packages\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    IntegerType, \n",
    "    DateType, \n",
    "    TimestampType,\n",
    "    )\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    min as Fmin, max as Fmax, \n",
    "    sum as Fsum, round as Fround, \n",
    "    \n",
    "    col, lit, split,\n",
    "    first, last, \n",
    "    desc, asc,\n",
    "    avg,\n",
    "    count, countDistinct, approx_count_distinct,\n",
    "    when, \n",
    "    isnull, isnan,\n",
    "    from_unixtime, date_format, datediff,\n",
    "    dayofmonth, month, hour, to_date,\n",
    "    )\n",
    "\n",
    "# libraries and packages for modeling\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder, \n",
    "    OneHotEncoderModel\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier,\n",
    "    MultilayerPerceptronClassifier,\n",
    "    LinearSVC\n",
    ")\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f7588f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7f185c82754e0bbcbf4ce39991790f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a Spark session using the SparkSession APIs\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Sparkify\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3dc77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b010c884e304dca9810b0f8594662db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a path variable for the full dataset file\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae556d3a",
   "metadata": {},
   "source": [
    "## Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4509f409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf07be631fc4c8783179f86cf0af37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the raw dataset in Spark.\n",
    "    \n",
    "    INPUT:\n",
    "            (str) - path for datafile\n",
    "    OUTPUT:\n",
    "            (PySpark dataframe) - dataframe of raw data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading the dataset ...\")\n",
    "    df = spark.read.json(file_path)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60ef9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e233efdc5a974051a6dd70dad9886d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Performs basic cleaning operations on the raw data.\n",
    "        - removes entries with missing userId\n",
    "        - split the location column in two columns: city and state\n",
    "        - parses timestamp columns\n",
    "    \n",
    "    INPUT:\n",
    "            (PySpark dataframe) - dataframe of raw data\n",
    "    OUTPUT:\n",
    "            (PySpark dataframe) - dataframe of cleaned data\n",
    "    \"\"\"\n",
    "    \n",
    "    # print message to indicate the start of the process\n",
    "    print(\"Cleaning the data ...\")\n",
    "    \n",
    "    # print a count of rows before cleaning\n",
    "    initial_records = df.count()\n",
    "    print(\"Dataset has {} rows initially.\".format(initial_records))\n",
    "    \n",
    "    # filter out all the records without userId\n",
    "    df = df.filter(df.userId != \"\")\n",
    "    \n",
    "    # split the location column in two columns: city and state\n",
    "    df = df.withColumn(\"city\", split(df[\"location\"], \",\").getItem(0)) \\\n",
    "           .withColumn(\"state\", split(df[\"location\"], \",\").getItem(1))\n",
    "\n",
    "    # drop the location column\n",
    "    df.drop(df.location)\n",
    "    \n",
    "    # parse the timestamp columns: registration and ts\n",
    "    df = df.withColumn(\"event_ts\", from_unixtime(col(\"ts\")/1000.0))\n",
    "    df = df.withColumn(\"reg_ts\", from_unixtime(col(\"registration\")/1000.0))\n",
    "    \n",
    "    # print end of process message\n",
    "    print(\"Finished cleaning the data ...\")\n",
    "    \n",
    "    # print a count of rows after cleaning\n",
    "    removed_rows = initial_records - df.count()\n",
    "    print(\"Cleaned dataset has {} rows, {} rows were removed\". \\\n",
    "        format(df.count(), initial_records - df.count()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4faa5985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca9ecd0444143b6b97e5d99c7f5e439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset ...\n",
      "Cleaning the data ...\n",
      "Dataset has 26259199 rows initially.\n",
      "Finished cleaning the data ...\n",
      "Cleaned dataset has 26259199 rows, 0 rows were removed"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "data_sparkify = load_data(event_data)\n",
    "# clean the dataset\n",
    "df = clean_data(data_sparkify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0943a7c",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82cee775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2f2ecf835a49ae9f65aa32fda731d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare the data for modeling via creating several features.\n",
    "     - firstevent_ts (timestamp) - first time an user is active\n",
    "     - lastevent_ts (timestamp) - last time an user is active\n",
    "     - date_ts (date type) - date from timestamp\n",
    "     - date_reg (timestamp) - registration date\n",
    "     - init_days_interv (int) - days between registration and first activity\n",
    "     - tenure_days_interv (int) - days between registration and last activity\n",
    "     - active_days (int) - days the user has some activity on the platform\n",
    "     - session_h (float) - session's duration in hours\n",
    "     \n",
    "     INPUT:\n",
    "         df (PySpark dataframe) - cleaned dataframe\n",
    "     OUTPUT:\n",
    "         df (PySpark dataframe) - dataframe with the listed features added\n",
    "    \"\"\"\n",
    "    \n",
    "    # create window: data grouped by userId, time ordered     \n",
    "    win_user = (W.partitionBy(\"userId\")\n",
    "            .orderBy(\"event_ts\")\n",
    "            .rangeBetween(W.unboundedPreceding, W.unboundedFollowing))\n",
    "    # create window: data grouped by sessionId and userId, time ordered\n",
    "    win_user_session = (W.partitionBy(\"sessionId\", \"userId\")\n",
    "                    .orderBy(\"event_ts\")\n",
    "                    .rangeBetween(W.unboundedPreceding, W.unboundedFollowing))\n",
    "    \n",
    "    # record the first time an user is active\n",
    "    df = df.withColumn(\"firstevent_ts\", first(col(\"event_ts\")).over(win_user))\n",
    "    # record the last time an user is active\n",
    "    df = df.withColumn(\"lastevent_ts\", last(col(\"event_ts\")).over(win_user))\n",
    "\n",
    "    # extract date from \"ts\" record\n",
    "    df = df.withColumn(\"date_ts\",to_date(\"event_ts\")) \n",
    "    # extract \"registration\" date\n",
    "    df = df.withColumn(\"date_reg\",to_date(\"reg_ts\")) \n",
    "\n",
    "    # warmup time = registration time to first event in days\n",
    "    df = df.withColumn(\"init_days_interv\", datediff(col(\"firstevent_ts\"), col(\"reg_ts\")))\n",
    "    # tenure time = registration time to last event in days\n",
    "    df = df.withColumn(\"tenure_days_interv\", datediff(col(\"lastevent_ts\"), col(\"reg_ts\")))\n",
    "    # active time =  days between the first event and the last event \n",
    "    df = df.withColumn(\"active_days\", approx_count_distinct(col(\"date_ts\")).over(win_user))\n",
    "\n",
    "    # create column that records the individual session's duration in hours\n",
    "    df = df.withColumn(\"session_h\",\n",
    "                     (last(df.ts).over(win_user_session) - first(df.ts) \\\n",
    "                      .over(win_user_session))/(1000*3600))  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2e3cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a3b9dff1054c06bcf641bd65d3d4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess mini dataset\n",
    "df_proc = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ad7eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafce633b9144b3eb161b740b3822110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- event_ts: string (nullable = true)\n",
      " |-- reg_ts: string (nullable = true)\n",
      " |-- firstevent_ts: string (nullable = true)\n",
      " |-- lastevent_ts: string (nullable = true)\n",
      " |-- date_ts: date (nullable = true)\n",
      " |-- date_reg: date (nullable = true)\n",
      " |-- init_days_interv: integer (nullable = true)\n",
      " |-- tenure_days_interv: integer (nullable = true)\n",
      " |-- active_days: long (nullable = false)\n",
      " |-- session_h: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "# the features in the preprocessed dataframe\n",
    "df_proc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dab77d",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72d4cdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be52a7b916042da82ca5394faa09eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_features(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Feature engineering to be used in modelling.\n",
    "    - nr_songs (int) - total number of songs user listened to\n",
    "    - nr_likes (int) - total number of \"Thumbs Up\" of the user\n",
    "    - nr_dislikes (int) - total number of \"Thumbs Down\" of the user\n",
    "    - nr_playlist (int) - number of songs added to the playlist\n",
    "    - nr_friends (int) - number of friends added through \"Add Friend\"\n",
    "    - nr_downgrades (int) - total number of visits to \"Downgrade\" page by the user\n",
    "    - nr_upgrades (int) - total number of visits to \"Upgrade\" page by the user\n",
    "    - nr_home (int) - total number of visits to \"Home\" page by the user\n",
    "    - nr_error (int) - total number of errors encountered by the user\n",
    "    - nr_settings (int) - total number of visits to \"Settings\" page by the user\n",
    "    - nr_ads (int) - total number of ads the user got\n",
    "    - nr_sessions (int) - number of sessions of the user\n",
    "    - n_acts (int) - total number of actions taken by the user\n",
    "    - avg_sess_h (float) - average session length in hours\n",
    "    - acts_per_session (float) - average number of actions per session for the user\n",
    "    - songs_per_session (float) - average numer of songs listened per session by the user\n",
    "    - ads_per_session (float) - average number of ads per session, received by user\n",
    "    - init_days_interv (int) - time interval in days from registration to the first action of the user\n",
    "    - tenure_days_interv (int) - time interval in days from registration to the last action of the user\n",
    "    - active_days (int) - number of days the user was active on the platform\n",
    "    \n",
    "    - gender (binary) - 1 for F (female), 0 for M (male)\n",
    "    - level (binary) - 1 for paid, 0 for free\n",
    "    - downgrade (binary) - 1 for \"Submit Downgrade\" page visit, 0 otherwise \n",
    "    - churn (binary) - 1 for \"CAncellation Confirmation\" page visit, 0 otherwise\n",
    "    \n",
    "    INPUT:\n",
    "        df (PySpark dataframe) - preprocessed dataframe\n",
    "        df_feats (PySpark datafrane) - dataframe that contains engineered features\n",
    "    \"\"\"\n",
    "    \n",
    "    df_feats = df.groupBy(\"userId\") \\\n",
    "        .agg(\n",
    "    \n",
    "            # count user's individual actions using all page visits\n",
    "    \n",
    "            count(when(col(\"page\") == \"NextSong\", True)).alias(\"nr_songs\"),\n",
    "    \n",
    "            count(when(col(\"page\") == \"Thumbs Up\", True)).alias(\"nr_likes\"),\n",
    "            count(when(col(\"page\") == \"Thumbs Down\", True)).alias(\"nr_dislikes\"),\n",
    "    \n",
    "            count(when(col(\"page\") == \"Add to Playlist\", True)).alias(\"nr_playlist\"),\n",
    "            count(when(col(\"page\") == \"Add Friend\", True)).alias(\"nr_friends\"),\n",
    "    \n",
    "            count(when(col(\"page\") == \"Downgrade\", True)).alias(\"nr_downgrades\"),\n",
    "            count(when(col(\"page\") == \"Upgrade\", True)).alias(\"nr_upgrades\"),\n",
    "    \n",
    "            count(when(col(\"page\") == \"Home\", True)).alias(\"nr_home\"),\n",
    "            count(when(col(\"page\") == \"Error\", True)).alias(\"nr_error\"),\n",
    "            count(when(col(\"page\") == \"Settings\", True)).alias(\"nr_settings\"),\n",
    "    \n",
    "            count(when(col(\"page\") == \"Roll Advert\", True)).alias(\"nr_ads\"),\n",
    "    \n",
    "            # compute the number of sessions a user is in\n",
    "            countDistinct(\"sessionId\").alias(\"nr_sessions\"),\n",
    "    \n",
    "            # find the total number of actions a user took\n",
    "            countDistinct(\"itemInSession\").alias(\"n_acts\"),\n",
    "    \n",
    "            # compute the average session length in hours\n",
    "            avg(col(\"session_h\")).alias(\"avg_sess_h\"),\n",
    "    \n",
    "            # compute the average number of page actions per sesssion\n",
    "            (countDistinct(\"itemInSession\")/countDistinct(\"sessionId\")).alias(\"acts_per_session\"),\n",
    "    \n",
    "            # compute the average number of songs per session\n",
    "            (count(when(col(\"page\") == \"NextSong\",\n",
    "                        True))/countDistinct(\"sessionId\")).alias(\"songs_per_session\"),\n",
    "    \n",
    "            # compute the average number of ads per session\n",
    "             (count(when(col(\"page\") == \"Roll Advert\",\n",
    "                         True))/countDistinct(\"sessionId\")).alias(\"ads_per_session\"),\n",
    "        \n",
    "            # days between registration and first activity\n",
    "            first(col(\"init_days_interv\")).alias(\"init_days_interv\"),\n",
    "            # the tenure time on the platform: from registration to last event in days\n",
    "            first(col(\"tenure_days_interv\")).alias(\"tenure_days_interv\"),\n",
    "            # number of days user visited the platform  \n",
    "            first(col(\"active_days\")).alias(\"active_days\"),\n",
    "    \n",
    "            # encode the gender 1 for F and 0 for M\n",
    "            first(when(col(\"gender\") == \"F\", 1).otherwise(0)).alias(\"gender\"),\n",
    "    \n",
    "            # encode the level (paid/free) according to the last record\n",
    "            last(when(col(\"level\") == \"paid\", 1).otherwise(0)).alias(\"level\"),\n",
    "    \n",
    "            # flag those users that downgraded \n",
    "            last(when(col(\"page\") == \"Downgrade\", 1).otherwise(0)).alias(\"downgrade\"),\n",
    "    \n",
    "            # create the churn column that records if the user cancelled \n",
    "            last(when(col(\"page\") == \"Cancellation Confirmation\", 1).otherwise(0)).alias(\"churn\"),\n",
    "            )\n",
    "    # drop the column \"userId\" \n",
    "    df_feats = df_feats.drop(\"userId\")       \n",
    "    \n",
    "    return df_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1542d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd52061593dc49ae81f10e5751bb5fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nr_songs: long (nullable = false)\n",
      " |-- nr_likes: long (nullable = false)\n",
      " |-- nr_dislikes: long (nullable = false)\n",
      " |-- nr_playlist: long (nullable = false)\n",
      " |-- nr_friends: long (nullable = false)\n",
      " |-- nr_downgrades: long (nullable = false)\n",
      " |-- nr_upgrades: long (nullable = false)\n",
      " |-- nr_home: long (nullable = false)\n",
      " |-- nr_error: long (nullable = false)\n",
      " |-- nr_settings: long (nullable = false)\n",
      " |-- nr_ads: long (nullable = false)\n",
      " |-- nr_sessions: long (nullable = false)\n",
      " |-- n_acts: long (nullable = false)\n",
      " |-- avg_sess_h: double (nullable = true)\n",
      " |-- acts_per_session: double (nullable = true)\n",
      " |-- songs_per_session: double (nullable = true)\n",
      " |-- ads_per_session: double (nullable = true)\n",
      " |-- init_days_interv: integer (nullable = true)\n",
      " |-- tenure_days_interv: integer (nullable = true)\n",
      " |-- active_days: long (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- downgrade: integer (nullable = true)\n",
      " |-- churn: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# build the features dataframe for mini dataset\n",
    "df_feats = build_features(df_proc)\n",
    "df_feats.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a8fb5",
   "metadata": {},
   "source": [
    "### Investigate null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ab4f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0234f90caf4ba783a105bd0f94cf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " nr_songs           | 0   \n",
      " nr_likes           | 0   \n",
      " nr_dislikes        | 0   \n",
      " nr_playlist        | 0   \n",
      " nr_friends         | 0   \n",
      " nr_downgrades      | 0   \n",
      " nr_upgrades        | 0   \n",
      " nr_home            | 0   \n",
      " nr_error           | 0   \n",
      " nr_settings        | 0   \n",
      " nr_ads             | 0   \n",
      " nr_sessions        | 0   \n",
      " n_acts             | 0   \n",
      " avg_sess_h         | 0   \n",
      " acts_per_session   | 0   \n",
      " songs_per_session  | 0   \n",
      " ads_per_session    | 0   \n",
      " init_days_interv   | 0   \n",
      " tenure_days_interv | 0   \n",
      " active_days        | 0   \n",
      " gender             | 0   \n",
      " level              | 0   \n",
      " downgrade          | 0   \n",
      " churn              | 0"
     ]
    }
   ],
   "source": [
    "# drop the null values\n",
    "df_feats=df_feats.na.drop()\n",
    "# check for null and missing values\n",
    "df_feats.select([count(when(isnan(c) | col(c).isNull(), c)) \\\n",
    "           .alias(c) for c in df_feats.columns]) \\\n",
    "           .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f21ffc",
   "metadata": {},
   "source": [
    "## EDA on Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5f77cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ecd5a9ff1f45a5aefe4603bfbf024d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----+\n",
      "|churn_gender|   0|   1|\n",
      "+------------+----+----+\n",
      "|           0|8995|8279|\n",
      "|           1|2656|2347|\n",
      "+------------+----+----+"
     ]
    }
   ],
   "source": [
    "# counts of churn/non_churn users grouped by gender\n",
    "df_feats.crosstab(\"churn\", \"gender\").sort(\"churn_gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "083fee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199df8b5fe8e4454bacbe084999dd530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+\n",
      "|churn_level|   0|    1|\n",
      "+-----------+----+-----+\n",
      "|          0|7029|10245|\n",
      "|          1|1579| 3424|\n",
      "+-----------+----+-----+"
     ]
    }
   ],
   "source": [
    "# counts of churn/non_churn users grouped by level\n",
    "df_feats.crosstab(\"churn\", \"level\").sort(\"churn_level\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dad17d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a6c9f9bb3145b19409c4f06416c888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|songs_per_session|\n",
      "+-------+-----------------+\n",
      "|  count|            22277|\n",
      "|   mean| 65.5269198794409|\n",
      "| stddev| 41.3827564482615|\n",
      "|    min|              0.0|\n",
      "|    max|            579.0|\n",
      "+-------+-----------------+"
     ]
    }
   ],
   "source": [
    "# display statistics for songs played per session\n",
    "df_feats.describe(\"songs_per_session\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c258b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41410f33756442f839739dbfaf3c455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|  acts_per_session|\n",
      "+-------+------------------+\n",
      "|  count|             22277|\n",
      "|   mean|29.963675940246336|\n",
      "| stddev| 33.17610291712757|\n",
      "|    min|               0.5|\n",
      "|    max|             693.0|\n",
      "+-------+------------------+"
     ]
    }
   ],
   "source": [
    "# display statistics for actions played per session\n",
    "df_feats.describe(\"acts_per_session\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eea61c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f879c083de48698c4b2f43d78a45a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|  ads_per_session|\n",
      "+-------+-----------------+\n",
      "|  count|            22277|\n",
      "|   mean|1.685658016042972|\n",
      "| stddev|2.047600133034228|\n",
      "|    min|              0.0|\n",
      "|    max|             36.0|\n",
      "+-------+-----------------+"
     ]
    }
   ],
   "source": [
    "# display statistics for average number of ads received per session\n",
    "df_feats.describe(\"ads_per_session\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b5f29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690a2561917c4597a19774ff814c8021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|         nr_likes|       nr_dislikes|      nr_upgrades|     nr_downgrades|\n",
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|            22277|             22277|            22277|             22277|\n",
      "|   mean|51.68851281590879| 10.73807065583337|2.267226287202047|  8.27041343089285|\n",
      "| stddev|64.58937093224218|12.616020453772165|2.653005101910216|11.600411115347786|\n",
      "|    min|                0|                 0|                0|                 0|\n",
      "|    max|              836|               154|               29|               133|\n",
      "+-------+-----------------+------------------+-----------------+------------------+"
     ]
    }
   ],
   "source": [
    "# statistics for the four features\n",
    "df_feats.describe([\"nr_likes\", \"nr_dislikes\", \"nr_upgrades\", \"nr_downgrades\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabcded",
   "metadata": {},
   "source": [
    "### Feature Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b93dfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09797d5f190443e8bb077ff21d14da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def median_diff(df, col, bincol):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the values in a column, computes the normalized difference between the medians \n",
    "    of the subsets based on values in a categorical column.\n",
    "    \n",
    "    INPUT:\n",
    "        df (Pandas dataframe) - dataframe that contains both columns\n",
    "        col (Pandas series) - column in df, of integer, float type\n",
    "        bincol (Pandas series) - column in df, with entries 0, 1\n",
    "    OUTPUT:\n",
    "        dictionary: key = nd_col, value = normalized_diff\n",
    "    \"\"\"\n",
    "    median_one = abs(df[df[bincol]==1][col].median())\n",
    "    median_zero = abs(df[df[bincol]==0][col].median())\n",
    "    normalized_difference = (median_zero - median_one)/max(median_zero, median_one)\n",
    "    return {\"column\": f\"nd_{col}\",\n",
    "            \"normal_diff\": normalized_difference}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d49e62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af3da2121ca4eb485ec17020ebd736b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   column  normal_diff\n",
      "0             nd_nr_error     1.000000\n",
      "1   nd_tenure_days_interv     0.291139\n",
      "2          nd_nr_sessions     0.200000\n",
      "3             nd_nr_likes     0.193548\n",
      "4          nd_active_days     0.181818\n",
      "5          nd_nr_playlist     0.125000\n",
      "6     nd_init_days_interv     0.121212\n",
      "7           nd_nr_friends     0.100000\n",
      "8             nd_nr_songs     0.090586\n",
      "9              nd_nr_home     0.037037\n",
      "10              nd_n_acts     0.022936\n",
      "11          nd_avg_sess_h     0.014503\n",
      "12         nd_nr_settings     0.000000\n",
      "13         nd_nr_upgrades     0.000000\n",
      "14   nd_songs_per_session    -0.031633\n",
      "15         nd_nr_dislikes    -0.142857\n",
      "16    nd_acts_per_session    -0.148629\n",
      "17     nd_ads_per_session    -0.161333\n",
      "18              nd_nr_ads    -0.166667\n",
      "19       nd_nr_downgrades    -0.200000"
     ]
    }
   ],
   "source": [
    "# list of numerical columns to investigate\n",
    "num_cols = df_feats.toPandas().columns[:20]\n",
    "\n",
    "# create a Pandas dataframe with the normalized median differences\n",
    "df_medians = pd.DataFrame(list(map(lambda col: median_diff(df_feats.toPandas(), col, \"churn\"), num_cols)))\n",
    "df_medians = df_medians.sort_values([\"normal_diff\"], ascending = False).reset_index(drop = True)\n",
    "df_medians.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972f6ab",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915a106",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06929dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1db73508024651bf88c27de147251b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SPLIT_VALS = [.7, .3]\n",
    "\n",
    "# split the data into train and test sets\n",
    "\n",
    "def split_data (df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split the dataset into training set and test set.\n",
    "    \"\"\"\n",
    "    train_set, test_set = df.randomSplit(SPLIT_VALS,seed=1234)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1eac6",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c35099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a58112eef534033beda71c1d90603bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split the features and the label\n",
    "CAT_FEATURES = [\"gender\", \"level\", \"downgrade\"]\n",
    "CONT_FEATURES = [\"nr_songs\", \"nr_likes\", \"nr_dislikes\", \"nr_playlist\", \"nr_friends\", \"nr_downgrades\",\n",
    "                \"nr_upgrades\", \"nr_home\", \"nr_error\", \"nr_settings\", \"nr_ads\", \"nr_sessions\",\n",
    "                \"n_acts\", \"avg_sess_h\", \"acts_per_session\", \"songs_per_session\", \"ads_per_session\",\n",
    "                \"init_days_interv\", \"tenure_days_interv\", \"active_days\"]\n",
    "CHURN_LABEL = \"churn\"\n",
    "\n",
    "\n",
    "def build_pipeline(classifier):\n",
    "    \"\"\"\n",
    "    Combines all the stages of the processing data. \n",
    "    \"\"\"\n",
    "    # stages in the pipeline\n",
    "    stages = [] \n",
    "    \n",
    "    # encode the labels\n",
    "    label_indexer =  StringIndexer(inputCol=CHURN_LABEL, outputCol=\"label\")\n",
    "    stages += [label_indexer]\n",
    "    \n",
    "    # encode the binary features\n",
    "    bin_assembler = VectorAssembler(inputCols=CAT_FEATURES, outputCol=\"bin_features\")\n",
    "    stages += [bin_assembler]\n",
    "    \n",
    "    # encode the continuous features\n",
    "    cont_assembler = VectorAssembler(inputCols = CONT_FEATURES, outputCol=\"cont_features\")\n",
    "    stages += [cont_assembler]\n",
    "    # normalize the continuous features\n",
    "    cont_scaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaler\", \n",
    "                                 withStd=True , withMean=True)\n",
    "    stages += [cont_scaler]\n",
    "    \n",
    "    # pass all to the vector assembler to create a single sparse vector\n",
    "    all_assembler = VectorAssembler(inputCols=[\"bin_features\", \"cont_scaler\"],  \n",
    "                            outputCol=\"features\")\n",
    "    stages += [all_assembler]\n",
    "    \n",
    "    # add the model to the pipeline\n",
    "    stages += [classifier]\n",
    "    \n",
    "    # create a pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35042a0c",
   "metadata": {},
   "source": [
    "### Build Individual Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ab96df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c39d44efc94dfba552b139c5a256f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train a Logistic Regression model\n",
    "def logistic_regression_pipeline():\n",
    "    lr_classifier = LogisticRegression(labelCol = \"label\",\n",
    "                                       featuresCol = \"features\")\n",
    "    return build_pipeline(lr_classifier)\n",
    "\n",
    "# train a Decision Tree model\n",
    "def decision_tree_pipeline():\n",
    "    dt_classifier = DecisionTreeClassifier(labelCol = \"label\",\n",
    "                                           featuresCol = \"features\",\n",
    "                                           seed=1234)\n",
    "    return build_pipeline(dt_classifier)\n",
    "\n",
    "# train a Random Forest model\n",
    "def random_forest_pipeline():\n",
    "    rf_classifier = RandomForestClassifier(labelCol = \"label\",\n",
    "                                           featuresCol = \"features\", \n",
    "                                           seed=1234)\n",
    "    return build_pipeline(rf_classifier)\n",
    "\n",
    "# train a Gradient-boosted Tree model\n",
    "def gradient_boosted_pipeline():\n",
    "    gbt_classifier = GBTClassifier(labelCol = \"label\",\n",
    "                                   featuresCol = \"features\",\n",
    "                                   seed=1234)\n",
    "    return build_pipeline(gbt_classifier)\n",
    "\n",
    "# train a Multilayer Perceptron Classifier\n",
    "def multilayer_perceptron_pipeline():\n",
    "    # specify layers: 23 (features), two intermediate (8, 4), output 2 (classes)\n",
    "    layers=[23, 8, 4, 2]\n",
    "    # create the trainer and set its parameters\n",
    "    mlpc_classifier = MultilayerPerceptronClassifier(labelCol = \"label\",\n",
    "                                                     featuresCol = \"features\",\n",
    "                                                     layers=layers,\n",
    "                                                     seed=1234)\n",
    "    return build_pipeline(mlpc_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40b6b2",
   "metadata": {},
   "source": [
    "### Build K-Fold Cross Validation and Grid Search Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "970a2e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b6459bbdea43498c7ee93ac2d7e477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# implement K-fold cross validation and grid search \n",
    "\n",
    "def grid_search_model(pipeline, param):\n",
    "    \"\"\"\n",
    "    Creates a cross validation object.\n",
    "    \n",
    "    INPUT:\n",
    "        param = grid of parameter\n",
    "        pipeline = model pipeline \n",
    "    \n",
    "    OUTPUT:\n",
    "        cv = cross validation object\n",
    "    \"\"\"\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=param,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5,\n",
    "                    parallelism=2)\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d58963f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0270c2596a814b15bcfe542778ee1f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"Logistic_Regression\"\n",
    "def lr_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Logistic Regression\n",
    "    param_lr = ParamGridBuilder()\n",
    "    param_lr = param_lr.addGrid(model.regParam, [.01, .1, .5])\n",
    "    param_lr = param_lr.addGrid(model.elasticNetParam, [.01, .5])\n",
    "    param_lr = param_lr.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_lr)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_lr)\n",
    "\n",
    "\n",
    "# model_name = \"Decision Trees\"\n",
    "def dt_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Decision Trees\n",
    "    param_dt = ParamGridBuilder()\n",
    "    param_dt = param_dt.addGrid(model.maxDepth, [5, 10, 15, 25])\n",
    "    param_dt = param_dt.addGrid(model.maxBins, [8, 16, 32])\n",
    "    param_dt = param_dt.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_dt)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_dt)\n",
    "\n",
    "\n",
    "# model_name = \"Random Forest\"\n",
    "def rf_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Random Forest\n",
    "    param_rf = ParamGridBuilder()\n",
    "    param_rf = param_rf.addGrid(model.maxDepth, [15, 25])\n",
    "    param_rf = param_rf.addGrid(model.maxBins, [8, 16, 32])\n",
    "    param_rf = param_rf.addGrid(model.numTrees, [40, 60])\n",
    "    param_rf = param_rf.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_rf)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_rf)\n",
    "\n",
    "\n",
    "# model_name = \"Gradient Boosted Trees\"\n",
    "def gbt_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Gradient Boosted Trees\n",
    "    param_gbt = ParamGridBuilder()\n",
    "    param_gbt = param_gbt.addGrid(model.maxDepth, [15, 25])\n",
    "    param_gbt = param_gbt.addGrid(model.maxIter, [10, 20])\n",
    "    param_gbt = param_gbt.addGrid(model.stepSize, [.05, .1])\n",
    "    param_gbt = param_gbt.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_gbt)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_gbt)\n",
    "\n",
    "\n",
    "# model_name = \"Multilayer Perceptron Classifier\"\n",
    "def mlpc_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    # create a list of parameters for Multilayer Perceptron\n",
    "    param_mlpc = ParamGridBuilder()\n",
    "    param_mlpc = param_mlpc.addGrid(model.stepSize, [.03, .05])\n",
    "    param_mlpc = param_mlpc.addGrid(model.maxIter, [128, 256])\n",
    "    param_mlpc = param_mlpc.build()\n",
    "    \n",
    "    print(f\"Models trained: {len(param_mlpc)}\")\n",
    "    \n",
    "    return grid_search_model(pipeline, param_mlpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb120394",
   "metadata": {},
   "source": [
    "### Build Model Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2e4bf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5a5a1d8ef24223abdf373bd894c77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# function to compute relevant metrics for binary classification\n",
    "def conf_metrics(dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "        Calculates the metrics associated to the confusion matrix.\n",
    "\n",
    "        INPUT:\n",
    "            dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "        OUTPUT:\n",
    "            accuracy (float) - metric\n",
    "            precision (float) - metric\n",
    "            recall (float) - metric\n",
    "            F1 (float) - metric\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # calculate the elements of the confusion matrix\n",
    "    tn = dataset.where((dataset[labelCol]==0) & (dataset[predCol]==0)).count()\n",
    "    tp = dataset.where((dataset[labelCol]==1) & (dataset[predCol]==1)).count()                   \n",
    "    fn = dataset.where((dataset[labelCol]==1) & (dataset[predCol]==0)).count()                   \n",
    "    fp = dataset.where((dataset[labelCol]==0) & (dataset[predCol]==1)).count()\n",
    "    \n",
    "    # calculate accuracy, precision, recall, and F1-score\n",
    "    accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 =  2 * (precision*recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a0115d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de00ccffa1e40d5a8510787c83c0112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_metrics(dataset, roc_cl, pr_cl):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints evaluation metrics for the model. \n",
    "    \n",
    "    INPUT:\n",
    "         dataset (pyspark.sql.DataFrame) - a dataset that contains\n",
    "                                labels and predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = conf_metrics(dataset)[0]\n",
    "    precision = conf_metrics(dataset)[1]\n",
    "    recall = conf_metrics(dataset)[2]\n",
    "    f1 = conf_metrics(dataset)[3]\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    dataset.groupBy(dataset[labelCol], dataset[predCol]).count().show()\n",
    "    print(\"\")\n",
    "    print(\"accuracy...............%6.3f\" % accuracy)\n",
    "    print(\"precision..............%6.3f\" % precision)\n",
    "    print(\"recall.................%6.3f\" % recall)\n",
    "    print(\"F1.....................%6.3f\" % f1)\n",
    "    print(\"auc_roc................%6.3f\" % roc_cl)\n",
    "    print(\"auc_pr.................%6.3f\" % pr_cl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6e6ca",
   "metadata": {},
   "source": [
    "## Train, Tune and Evaluate Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4019af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e691d8359b894bad9c4e501461840ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split the data in training set and test set\n",
    "train_set, test_set = split_data(df_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7105710",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9d0f738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5401249f5c8148f8b03719e091f7f3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-31:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LOGISTIC REGRESSION CLASSIFIER\n",
      "\n",
      "Models trained: 6\n",
      "\n",
      "Training time.........23.539 min\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|  597|\n",
      "|  0.0|       1.0|  129|\n",
      "|  1.0|       0.0|  952|\n",
      "|  0.0|       0.0| 4912|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "accuracy............... 0.836\n",
      "precision.............. 0.822\n",
      "recall................. 0.385\n",
      "F1..................... 0.525\n",
      "auc_roc................ 0.879\n",
      "auc_pr................. 0.701\n",
      "\n",
      "The best hyperparameter values from the grid:\n",
      "maxDepth:.......... 0.01\n",
      "maxBins:........... 0.5"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(f\"Training LOGISTIC REGRESSION CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# build specific pipeline\n",
    "pipeline = logistic_regression_pipeline()\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "lr = lr_grid_search(pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_lr = lr.fit(train_set)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_lr = model_lr.bestModel.transform(test_set)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_lr = evaluator.evaluate(predictions_lr, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_lr = evaluator.evaluate(predictions_lr, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_lr, prec_lr, rec_lr, f1_lr = conf_metrics(predictions_lr)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "display_metrics(predictions_lr, roc_lr, pr_lr)\n",
    "print(\"\")\n",
    "\n",
    "best_param_lr = list(model_lr.getEstimatorParamMaps()[np.argmax(model_lr.avgMetrics)].values())\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"maxDepth:..........\", best_param_lr[0])\n",
    "print(\"maxBins:...........\", best_param_lr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b46e4",
   "metadata": {},
   "source": [
    "### Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01de2b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c7a2fc7e29495cbcdc08ac7850c18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-32:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DECISION TREES CLASSIFIER\n",
      "\n",
      "Models trained: 12\n",
      "\n",
      "Training time.........23.727 min\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0| 1392|\n",
      "|  0.0|       1.0|  196|\n",
      "|  1.0|       0.0|  157|\n",
      "|  0.0|       0.0| 4845|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "accuracy............... 0.946\n",
      "precision.............. 0.877\n",
      "recall................. 0.899\n",
      "F1..................... 0.887\n",
      "auc_roc................ 0.768\n",
      "auc_pr................. 0.663\n",
      "\n",
      "The best hyperparameter values from the grid:\n",
      "maxDepth:.......... 25\n",
      "maxBins:........... 32"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(f\"Training DECISION TREES CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# build specific pipeline\n",
    "pipeline = decision_tree_pipeline()\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "dt = dt_grid_search(pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_dt = dt.fit(train_set)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_dt = model_dt.bestModel.transform(test_set)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_dt = evaluator.evaluate(predictions_dt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_dt = evaluator.evaluate(predictions_dt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_dt, prec_dt, rec_dt, f1_dt = conf_metrics(predictions_dt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "display_metrics(predictions_dt, roc_dt, pr_dt)\n",
    "print(\"\")\n",
    "\n",
    "best_param_dt = list(model_dt.getEstimatorParamMaps()[np.argmax(model_dt.avgMetrics)].values())\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"maxDepth:..........\", best_param_dt[0])\n",
    "print(\"maxBins:...........\", best_param_dt[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f72b3",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "906bfb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee895a85c3ee42ce8d2f92dab3976b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-33:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 11678\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RANDOM FOREST CLASSIFIER\n",
      "\n",
      "Models trained: 12\n",
      "\n",
      "Training time.........162.999 min\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0| 1068|\n",
      "|  0.0|       1.0|   50|\n",
      "|  1.0|       0.0|  481|\n",
      "|  0.0|       0.0| 4991|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "accuracy............... 0.919\n",
      "precision.............. 0.955\n",
      "recall................. 0.689\n",
      "F1..................... 0.801\n",
      "auc_roc................ 0.878\n",
      "auc_pr................. 0.733\n",
      "\n",
      "The best hyperparameter values from the grid:\n",
      "maxDepth:.......... 15\n",
      "maxBins:........... 32\n",
      "numTrees:.......... 60"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(f\"Training RANDOM FOREST CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# build specific pipeline\n",
    "pipeline = random_forest_pipeline()\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "rf = rf_grid_search(pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_rf = rf.fit(train_set)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_rf = model_rf.bestModel.transform(test_set)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_rf, prec_rf, rec_rf, f1_rf = conf_metrics(predictions_rf)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "display_metrics(predictions_rf, roc_rf, pr_rf)\n",
    "print(\"\")\n",
    "\n",
    "best_param_rf = list(model_rf.getEstimatorParamMaps()[np.argmax(model_rf.avgMetrics)].values())\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"maxDepth:..........\", best_param_rf[0])\n",
    "print(\"maxBins:...........\", best_param_rf[1])\n",
    "print(\"numTrees:..........\", best_param_rf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf219ce",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "740ffd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec046b0ec91e476f85bc507023763d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-34:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 18679\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GRADIENT BOOSTED TREES\n",
      "\n",
      "Models trained: 8\n",
      "\n",
      "Training time.........395.392 min\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0| 1394|\n",
      "|  0.0|       1.0|  153|\n",
      "|  1.0|       0.0|  155|\n",
      "|  0.0|       0.0| 4888|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "accuracy............... 0.953\n",
      "precision.............. 0.901\n",
      "recall................. 0.900\n",
      "F1..................... 0.901\n",
      "auc_roc................ 0.862\n",
      "auc_pr................. 0.719\n",
      "\n",
      "The best hyperparameter values from the grid:\n",
      "maxDepth:.......... 15\n",
      "maxIter:........... 20\n",
      "stepSize:.......... 0.1"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(f\"Training GRADIENT BOOSTED TREES\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# build specific pipeline\n",
    "pipeline = gradient_boosted_pipeline()\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "gbt = gbt_grid_search(pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_gbt = gbt.fit(train_set)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_gbt = model_gbt.bestModel.transform(test_set)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_gbt, prec_gbt, rec_gbt, f1_gbt = conf_metrics(predictions_gbt)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "display_metrics(predictions_gbt, roc_gbt, pr_gbt)\n",
    "print(\"\")\n",
    "\n",
    "best_param_gbt = list(model_gbt.getEstimatorParamMaps()[np.argmax(model_gbt.avgMetrics)].values())\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"maxDepth:..........\", best_param_gbt[0])\n",
    "print(\"maxIter:...........\", best_param_gbt[1])\n",
    "print(\"stepSize:..........\", best_param_gbt[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22bb471",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"Training MULTILAYER PERCEPTRON CLASSIFIER\")\n",
    "print(\"\")\n",
    "\n",
    "predCol=\"prediction\"\n",
    "labelCol=\"label\"\n",
    "\n",
    "# build specific pipeline\n",
    "pipeline = multilayer_perceptron_pipeline()\n",
    "\n",
    "# choose an evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(labelCol)\n",
    "\n",
    "# build the grid search pipeline\n",
    "mlpc = mlpc_grid_search(pipeline)\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "model_mlpc = mlpc.fit(train_set)\n",
    "\n",
    "# stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate the trainining time in minutes \n",
    "train_time = (end_time - start_time)/60\n",
    "\n",
    "# print the training time\n",
    "print(\"\")\n",
    "print(\"Training time.........%6.3f min\" % train_time)\n",
    "\n",
    "# create the predictions dataset\n",
    "predictions_mlpc = model_mlpc.bestModel.transform(test_set)\n",
    "\n",
    "# calculate auc metrics\n",
    "roc_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_mlpc = evaluator.evaluate(predictions_mlpc, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# record the confusion matrix metrics\n",
    "acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc = conf_metrics(predictions_mlpc)\n",
    "\n",
    "# print all evaluation metrics\n",
    "print(\"\")\n",
    "display_metrics(predictions_mlpc, roc_mlpc, pr_mlpc)\n",
    "print(\"\")\n",
    "\n",
    "best_param_mlpc = list(model_mlpc.getEstimatorParamMaps()[np.argmax(model_mlpc.avgMetrics)].values())\n",
    "print(\"The best hyperparameter values from the grid:\")\n",
    "print(\"stepSize:..........\", best_param_mlpc[0])\n",
    "print(\"maxIter:......... ..\", best_param_mlpc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebafda3",
   "metadata": {},
   "source": [
    "## Discuss the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c6316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4562a359a5e4857bd00bd57cbe8db13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              LinReg  DecTree  RandForest  GradBoost  MultiLPerc\n",
      "list_metrics                                                    \n",
      "accuracy       0.836    0.946       0.919      0.953       0.906\n",
      "precision      0.822    0.877       0.955      0.901       0.855\n",
      "recall         0.385    0.899       0.689      0.900       0.720\n",
      "f1_score       0.525    0.887       0.801      0.901       0.782\n",
      "auc_roc        0.879    0.768       0.878      0.862       0.923\n",
      "auc_pr         0.701    0.663       0.733      0.719       0.839"
     ]
    }
   ],
   "source": [
    "# create Pandas dataframe with metrics\n",
    "dict_metrics = {\"LinReg\": [acc_lr, prec_lr, rec_lr, f1_lr, roc_lr, pr_lr],\n",
    "                \"DecTree\": [acc_dt, prec_dt, rec_dt, f1_dt, roc_dt, pr_dt], \n",
    "                \"RandForest\": [acc_rf, prec_rf, rec_rf, f1_rf, roc_rf, pr_rf],\n",
    "                \"GradBoost\": [acc_gbt, prec_gbt, rec_gbt, f1_gbt, roc_gbt, pr_gbt],\n",
    "                \"MultiLPerc\": [acc_mlpc, prec_mlpc, rec_mlpc, f1_mlpc, roc_mlpc, pr_mlpc],\n",
    "                \"list_metrics\" : [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc_roc\", \"auc_pr\"]\n",
    "               }\n",
    "df_mets = pd.DataFrame.from_dict(dict_metrics).set_index(\"list_metrics\")\n",
    "df_mets.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fef5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
